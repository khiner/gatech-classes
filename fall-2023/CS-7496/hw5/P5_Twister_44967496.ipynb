{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVKyVSEx0E2t"
      },
      "source": [
        "# CS 4496/7496 Computer Animation (Fall 2023)\n",
        "Copyright (c) Georgia Institute of Technology\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Mj7GfIV0P3b"
      },
      "source": [
        "# Project 5: Twister (due 12/5/2023, Tuesday, 11:59PM)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A9VyPdYlzUo"
      },
      "source": [
        "![TWISER-IMAGE](https://upload.wikimedia.org/wikipedia/en/0/09/1966_Twister_Cover.jpg)\n",
        "\n",
        "Have you played Twister? Your body is basically solving an inverse kinematics problem when you are playing Twister. Please refer to the video if you are not too familiar with it (https://www.youtube.com/watch?v=7A5XO0udmdo).\n",
        "\n",
        "In this project, you will build a virtual Twister game by developing an IK solver."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RCDhidnTJZK"
      },
      "source": [
        "Let us install the required libraries before we begin.\n",
        "Similar to the previous assignments, we will be using PyBullet. (https://docs.google.com/document/d/10sXEhzFRSnvFcl3XxNGhnD4N2SedqwdAvK3dsihxVUA/edit#)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcLd3jk66YHC"
      },
      "outputs": [],
      "source": [
        "#@title Install dependencies\n",
        "!apt-get install -y xvfb python-opengl ffmpeg\n",
        "!pip install https://pybullet.org/download/pybullet-3.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
        "!pip install ffmpeg-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gldF3u6tDx0B"
      },
      "source": [
        "# 1. Set up transformations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlSAHnw7TSSK"
      },
      "outputs": [],
      "source": [
        "#@title Import required libraries\n",
        "import pybullet as p\n",
        "import numpy as np\n",
        "from math import sin, cos\n",
        "import scipy\n",
        "from scipy.spatial.transform import Rotation as R\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "import os\n",
        "import io\n",
        "from matplotlib import pyplot as plt\n",
        "import base64\n",
        "import ffmpeg\n",
        "from PIL import Image\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "# !wget \"https://lh3.googleusercontent.com/drive-viewer/AK7aPaDMGbPI4WXfr9GBmMOGPiuTmOkomDPeXVKgrXzefHIiFpOQuXO8ZY1j3TG8aipbi067Q5WFqj4yn1WmS2SkJYGcQTeIww=s2560\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyB_sbjpTi5_"
      },
      "source": [
        "Next, we define the transformations that we will make use of. We will first define a base class, followed by the fixed (0 DOF) and translation (1 DOF) transformations that inherit from this base class.\n",
        "\n",
        "For simplicity, we do not have a joint with multiple degrees of freedom. This is why our translation and rotation transformations only involve one axis (X/Y/Z) per object. We will instead use multiple objects of these types, one for each axis desired, to account for the degrees of freedom we need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uMT7LXy6TknI"
      },
      "outputs": [],
      "source": [
        "#@title `Transformation` base class, subclasses, utilities\n",
        "\n",
        "def transform_between(start, end):\n",
        "  \"\"\"This is a helper function to compute the transformation between start and\n",
        "  end transformations.\n",
        "  \"\"\"\n",
        "  if start == end:\n",
        "    return np.identity(4)\n",
        "  else:\n",
        "    return transform_between(start, end.parent) @ end.local_transform()\n",
        "\n",
        "\n",
        "class Transformation(object):\n",
        "  \"\"\" Base class for all transformations that we subsequently define. \"\"\"\n",
        "  def __init__(self, name=None):\n",
        "    global transform_list\n",
        "    transform_list.append(self)\n",
        "    self.name = name\n",
        "    self.parent = None  # If parent is none, it is a root node.\n",
        "    self.children = list()\n",
        "    self.dependent_transformations = set()\n",
        "\n",
        "  def add(self, child):\n",
        "    child.parent = self\n",
        "    self.children.append(child)\n",
        "    return child\n",
        "\n",
        "  def global_transform(self):\n",
        "    return transform_between(None, self)\n",
        "\n",
        "  def global_position(self):\n",
        "    return self.global_transform()[:3, 3]\n",
        "\n",
        "  def local_transform(self):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def local_derivative(self):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def num_dofs(self):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def get_dof_value(self):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def set_dof_value(self, value=0.0):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def __repr__(self):\n",
        "    return \"%s(%s)\" % (self.__class__.__name__, self.name)\n",
        "\n",
        "\n",
        "class Fixed(Transformation):\n",
        "  \"\"\" Fixed transformation. \"\"\"\n",
        "  def __init__(self, name, tx=0.0, ty=0.0, tz=0.0, rx=0.0, ry=0.0, rz=0.0):\n",
        "    super().__init__(name=name)\n",
        "    self._value = np.identity(4)\n",
        "    self._value[:3, :3] = R.from_euler('xyz', [rx, ry, rz]).as_matrix()\n",
        "    self._value[:3, 3] = (tx, ty, tz)\n",
        "\n",
        "  def num_dofs(self):\n",
        "    return 0\n",
        "\n",
        "  def local_transform(self):\n",
        "    return self._value\n",
        "\n",
        "\n",
        "class Translation(Transformation):\n",
        "  \"\"\" Class for translation. \"\"\"\n",
        "  def __init__(self, name, axis, value=0.0):\n",
        "    super().__init__(name=name)\n",
        "    self._value = value\n",
        "    self.axis = axis\n",
        "    self.axis_index = \"xyz\".find(axis)\n",
        "    self.T = np.identity(4)\n",
        "    self.dT = np.zeros((4, 4))\n",
        "\n",
        "  def num_dofs(self):\n",
        "    return 1\n",
        "\n",
        "  def get_dof_value(self):\n",
        "    return self._value\n",
        "\n",
        "  def set_dof_value(self, value=0.0):\n",
        "    self._value = value\n",
        "\n",
        "  def local_transform(self):\n",
        "    self.T[self.axis_index, 3] = self._value\n",
        "    return self.T\n",
        "\n",
        "  def local_derivative(self):\n",
        "    self.dT[self.axis_index, 3] = 1.0\n",
        "    return self.dT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9_7zNytTqBy"
      },
      "source": [
        "## 1.1 Hinge transformation.\n",
        "\n",
        " We have already defined the base class, the fixed transformation, and the translation transformation. Your task is to complete the rotation transformation.\n",
        "\n",
        " Please implement the `local_transform()` and `local_derivative()` methods as described in their docstrings.\n",
        "\n",
        " As per the slides, the derivative here is taken with respect to the single free variable of the transformation (as it has 1 DOF). So, for the hinge transformation's `local_derivative()` method, you should return the derivative with respect to the angle of rotation `_theta`.\n",
        "\n",
        " **<font color='orange'> \\*\\* Tasks 1 and 2: write your code below (10 + 10 = 20 pts) \\*\\* </font>**\n",
        "   \n",
        "**<font color='orange'> \\*\\* Note: The implementations of Task 1 and 2 are given. However, we recommend students to read the code carefully. \\*\\* </font>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfON8JFETtv1"
      },
      "outputs": [],
      "source": [
        "class Hinge(Transformation):\n",
        "  \"\"\"Class for rotation. \"\"\"\n",
        "  def __init__(self, name, axis, theta=0.0):\n",
        "    super().__init__(name=name)\n",
        "    self._theta = theta\n",
        "    self.axis = axis\n",
        "    self.axis_index = \"xyz\".find(axis)\n",
        "\n",
        "  def num_dofs(self):\n",
        "    return 1\n",
        "\n",
        "  def get_dof_value(self):\n",
        "    return self._theta\n",
        "\n",
        "  def set_dof_value(self, value=0.0):\n",
        "    self._theta = value\n",
        "\n",
        "  def local_transform(self):\n",
        "    \"\"\" This function is used to compute the transformation matrix for a\n",
        "    rotation operation. You are supposed to make use of the self._theta\n",
        "    parameter and the sin and cos functions which have already been imported\n",
        "    directly as `from math import sin, cos`.\n",
        "    You will have 3 cases depending on which axis the rotation is about, which\n",
        "    you can obtain from either self.axis ('x', 'y', z') or self.axis_index\n",
        "    (0, 1, 2).\n",
        "\n",
        "    Returns:\n",
        "      transform: The 4x4 transformation matrix.\n",
        "    \"\"\"\n",
        "    cth, sth = cos(self._theta), sin(self._theta)\n",
        "    # Student code starts here. Implement three transformation matrices for each case.\n",
        "    if self.axis == 'x':\n",
        "      return np.array([[1.0, 0.0, 0.0, 0.0],\n",
        "                       [0.0, cth, -sth, 0.0],\n",
        "                       [0.0, sth, cth, 0.0],\n",
        "                       [0.0, 0.0, 0.0, 1.0]])\n",
        "    elif self.axis == 'y':\n",
        "      return np.array([[cth, 0.0, sth, 0.0],\n",
        "                       [0.0, 1.0, 0.0, 0.0],\n",
        "                       [-sth, 0.0, cth, 0.0],\n",
        "                       [0.0, 0.0, 0.0, 1.0]])\n",
        "    elif self.axis == 'z':\n",
        "      return np.array([[cth, -sth, 0.0, 0.0],\n",
        "                      [sth, cth, 0.0, 0.0],\n",
        "                      [0.0, 0.0, 1.0, 0.0],\n",
        "                      [0.0, 0.0, 0.0, 1.0]])\n",
        "    # Student code ends here\n",
        "\n",
        "  def local_derivative(self):\n",
        "    \"\"\" This function is used to compute the derivative of the rotation matrix.\n",
        "    As before, you are supposed to make use of the self._theta parameter and the\n",
        "    sin and cos functions. Similar to the function above, you will have 3 cases\n",
        "    depending on the axis of rotation, which can be obtained using either\n",
        "    self.axis or self.axis_index.\n",
        "\n",
        "    Returns:\n",
        "      deriv_transform: The 4x4 derivative matrix.\n",
        "    \"\"\"\n",
        "    # Student code starts here. Implement the three derivative matrices.\n",
        "    cth, sth = cos(self._theta), sin(self._theta)\n",
        "    if self.axis == 'x':\n",
        "      return np.array([[0.0, 0.0, 0.0, 0.0],\n",
        "                       [0.0, -sth, -cth, 0.0],\n",
        "                       [0.0, cth, -sth, 0.0],\n",
        "                       [0.0, 0.0, 0.0, 0.0]])\n",
        "    elif self.axis == 'y':\n",
        "      return np.array([[-sth, 0.0, cth, 0.0],\n",
        "                       [0.0, 0.0, 0.0, 0.0],\n",
        "                       [-cth, 0.0, -sth, 0.0],\n",
        "                       [0.0, 0.0, 0.0, 0.0]])\n",
        "    elif self.axis == 'z':\n",
        "      return np.array([[-sth, -cth, 0.0, 0.0],\n",
        "                      [cth, -sth, 0.0, 0.0],\n",
        "                      [0.0, 0.0, 0.0, 0.0],\n",
        "                      [0.0, 0.0, 0.0, 0.0]])\n",
        "    # Student code ends here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "920NA-ZGmE66"
      },
      "source": [
        "Let us test our implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgQ8FSRdmD7i"
      },
      "outputs": [],
      "source": [
        "#@title `Hinge` class unit tests\n",
        "\n",
        "transform_list = list()\n",
        "print(\"Test rotation transformation for x-axis with theta = 75 degrees\")\n",
        "hingeTest0 = Hinge(\"test0\", axis=\"x\", theta=(5/12 * np.pi)) # 75 degrees\n",
        "print(\"Testing local_transform()\")\n",
        "ansLocalTransform0 = np.array([[ 1., 0., 0., 0.],\n",
        " [0., 0.258819, -0.965926, 0.],\n",
        " [0., 0.965926, 0.258819, 0.],\n",
        " [0., 0., 0., 1.]])\n",
        "assert np.allclose(ansLocalTransform0, hingeTest0.local_transform(), atol=1e-5)\n",
        "print(\"Passed local_transform() for test0!\")\n",
        "print(\"Testing local_derivative()\")\n",
        "ansLocalDerivative0 = np.array([[ 0., 0., 0., 0.],\n",
        " [0., -0.965926, -0.258819, 0.],\n",
        " [0., 0.258819, -0.965926, 0.],\n",
        " [0., 0., 0., 0.]])\n",
        "assert np.allclose(ansLocalDerivative0, hingeTest0.local_derivative(), atol=1e-5)\n",
        "print(\"Passed local_derivative() for test0!\")\n",
        "print(\"-----------------------\")\n",
        "\n",
        "print(\"Test rotation transformation for y-axis with theta = 45 degrees\")\n",
        "hingeTest1 = Hinge(\"test1\", axis=\"y\", theta=(1/4 * np.pi)) # 45 degrees\n",
        "print(\"Testing local_transform()\")\n",
        "ansLocalTransform1 = np.array([[0.707107, 0., 0.707107, 0.],\n",
        " [0., 1., 0., 0.],\n",
        " [-0.707107, 0., 0.707107, 0.],\n",
        " [0., 0., 0., 1.]])\n",
        "assert np.allclose(ansLocalTransform1, hingeTest1.local_transform(), atol=1e-5)\n",
        "print(\"Passed local_transform() for test1!\")\n",
        "print(\"Testing local_derivative()\")\n",
        "ansLocalDerivative1 = np.array([[-0.707107, 0., 0.707107, 0.],\n",
        " [0., 0., 0., 0.],\n",
        " [-0.707107, 0., -0.707107, 0.],\n",
        " [0., 0., 0., 0.]])\n",
        "assert np.allclose(ansLocalDerivative1, hingeTest1.local_derivative(), atol=1e-5)\n",
        "print(\"Passed local_derivative() for test1!\")\n",
        "print(\"-----------------------\")\n",
        "\n",
        "print(\"Test rotation transformation for z-axis with theta = 60 degrees\")\n",
        "hingeTest2 = Hinge(\"test2\", axis=\"z\", theta=(1/3 * np.pi)) # 60 degrees\n",
        "print(\"Testing local_transform()\")\n",
        "ansLocalTransform2 = np.array([[ 0.5, -0.866025, 0., 0.],\n",
        " [0.8660254, 0.5, 0., 0.],\n",
        " [0., 0., 1., 0.],\n",
        " [0., 0., 0., 1.]])\n",
        "assert np.allclose(ansLocalTransform2, hingeTest2.local_transform(), atol=1e-5)\n",
        "print(\"Passed local_transform() for test2!\")\n",
        "print(\"Testing local_derivative()\")\n",
        "ansLocalDerivative2 = np.array([[-0.866025, -0.5, 0., 0.],\n",
        " [0.5, -0.866025, 0., 0.],\n",
        " [0., 0., 0., 0.],\n",
        " [0., 0., 0., 0.]])\n",
        "assert np.allclose(ansLocalDerivative2, hingeTest2.local_derivative(), atol=1e-5)\n",
        "print(\"Passed local_derivative() for test2!\")\n",
        "print(\"-----------------------\")\n",
        "\n",
        "print(\"Passed all tests!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42kw_j5uESO1"
      },
      "source": [
        "# 2. Set up the skeleton."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtT2pizrVI1f"
      },
      "source": [
        "We will now set up the structure of character, also called the skeleton."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRUVX6EyVP5q"
      },
      "outputs": [],
      "source": [
        "#@title `setup_character()`: Skeleton setup\n",
        "\n",
        "# Global structure\n",
        "transform_list = list()\n",
        "name_to_transform = dict()\n",
        "transform_list_1dof = list()\n",
        "motion = list()\n",
        "\n",
        "\n",
        "def setup_character():\n",
        "  global transform_list, name_to_transform, transform_list_1dof\n",
        "  # Reset the data structure\n",
        "  transform_list = list()\n",
        "  name_to_transform = dict()\n",
        "  transform_list_1dof = list()\n",
        "  motion = list()\n",
        "\n",
        "  # Define the character\n",
        "  base = Translation(\"j_tx\", axis=\"x\").add(Translation(\"j_ty\", axis=\"y\")).add(Translation(\"j_tz\", axis=\"z\"))\\\n",
        "    .add(Hinge(\"j_rx\", axis=\"x\")).add(Hinge(\"j_ry\", axis=\"y\")).add(Hinge(\"j_rz\", axis=\"z\"))\\\n",
        "    .add(Fixed(\"base\"))\n",
        "  base.add(Fixed(\"head\", tz=0.5))\n",
        "  for d in ['l', 'r']:\n",
        "    sx = 1.0 if d == 'l' else -1.0\n",
        "    base.add(Fixed(\"%s_base_to_shoulder\" % d, tx=sx*0.2, tz=0.25))\\\n",
        "      .add(Hinge(\"j_%s_shoulder_x\" % d, \"x\", 0.0)).add(Hinge(\"j_%s_shoulder_y\" % d, \"y\", 0.0)).add(Hinge(\"j_%s_shoulder_z\" % d, \"z\", 0.0))\\\n",
        "      .add(Fixed(\"%s_upperarm1\" % d, tx=sx*0.15)).add(Fixed(\"%s_upperarm2\" % d, tx=sx*0.15))\\\n",
        "      .add(Hinge(\"j_%s_elbow\" % d , \"z\", 0.0)).add(Fixed(\"%s_lowerarm1\" % d, tx=sx*0.15)).add(Fixed(\"%s_lowerarm2\" % d, tx=sx*0.15))\\\n",
        "      .add(Fixed('%s_hand' % d, tx=sx*0.1))\n",
        "  for d in ['l', 'r']:\n",
        "    sx = 1.0 if d == 'l' else -1.0\n",
        "    base.add(Fixed(\"%s_base_to_hip\" % d, tx=sx*0.13, tz=-0.3))\\\n",
        "      .add(Hinge(\"j_%s_hip_x\" % d, \"x\", 0.0)).add(Hinge(\"j_%s_hip_y\" % d, \"y\", 0.0)).add(Hinge(\"j_%s_hip_z\" % d, \"z\", 0.0))\\\n",
        "      .add(Fixed(\"%s_thigh1\" % d, tz=-0.2)).add(Fixed(\"%s_thigh2\" % d, tz=-0.2))\\\n",
        "      .add(Hinge(\"j_%s_knee\" % d , \"x\", 0.0)).add(Fixed(\"%s_shin1\" % d, tz=-0.2)).add(Fixed(\"%s_shin2\" % d, tz=-0.2))\\\n",
        "      .add(Hinge(\"j_%s_ankle\" % d , \"x\", 0.0)).add(Fixed(\"%s_feet\" % d, ty=-0.12))\n",
        "\n",
        "  # Build auxiliary data structures\n",
        "  for tr in transform_list:\n",
        "    name_to_transform[tr.name] = tr\n",
        "    if tr.num_dofs() == 1:\n",
        "      transform_list_1dof.append(tr)\n",
        "\n",
        "  # Build dependent transformations.\n",
        "  # Each joint depends on itself (if it has a DOF) as well as all its ancestors with DOFs in the skeleton hierarchy.\n",
        "  for tr in transform_list:\n",
        "    tr.dependent_transformations = tr.parent.dependent_transformations.copy() if tr.parent else set()\n",
        "\n",
        "    if tr.num_dofs() == 1:\n",
        "      tr.dependent_transformations.add(tr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSF9HDf3VUFz"
      },
      "source": [
        "Let us now visualize our skeleton using PyBullet. In order to do this, we first define a base class for Rigid bodies. We then define classes for the various parts of our skeleton, namely the Box and Sphere classes, which inherit from the RigidBody class. We then use each of these components to setup the skeleton."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hzVVQ4UVfrP"
      },
      "outputs": [],
      "source": [
        "#@title Rigidbody setup & visualization (PyBullet)\n",
        "\n",
        "p.connect(p.DIRECT)\n",
        "p.resetSimulation()\n",
        "\n",
        "pixelWidth = 640\n",
        "pixelHeight = 360\n",
        "viewMatrix = p.computeViewMatrixFromYawPitchRoll(cameraTargetPosition=[0,0,-0.3], distance=2.0, yaw=0.0, pitch=0.0, roll=0.0, upAxisIndex=2)\n",
        "projectionMatrix = p.computeProjectionMatrixFOV(fov=60, aspect=pixelWidth / pixelHeight, nearVal=0.01, farVal=100)\n",
        "\n",
        "body_list = list()\n",
        "target_body_list = list()\n",
        "\n",
        "class RigidBody(object):\n",
        "  \"\"\"\n",
        "  A base class for rigid bodies.\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "      transform):\n",
        "    global body_list\n",
        "    self.transform = transform\n",
        "    self.uid = -1\n",
        "    body_list.append(self)\n",
        "\n",
        "\n",
        "  def get_position_and_orientation(self):\n",
        "    T = self.transform.global_transform()\n",
        "    position = T[:3, 3]\n",
        "    orientation = R.from_matrix(T[:3, :3]).as_quat()\n",
        "    return position, orientation\n",
        "\n",
        "  def update_pybullet(self):\n",
        "    position, orientation = self.get_position_and_orientation()\n",
        "    p.resetBasePositionAndOrientation(self.uid, position, orientation)\n",
        "\n",
        "\n",
        "class Box(RigidBody):\n",
        "  \"\"\" We use this class, which inherits from the RigidBody class, to create the\n",
        "  torso and limbs of our character.\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "      transform,\n",
        "      color = np.array([0, 1, 1, 1]),\n",
        "      half_extends = [1, 1, 1],\n",
        "      texUid = None):\n",
        "    super().__init__(transform)\n",
        "\n",
        "    self.visualId = p.createVisualShape(p.GEOM_BOX, halfExtents=half_extends, rgbaColor=color, specularColor=[1, 1, 1])\n",
        "    position, orientation = self.get_position_and_orientation()\n",
        "    self.uid = p.createMultiBody(1.0, -1, self.visualId, position, orientation)\n",
        "    if texUid:\n",
        "      p.changeVisualShape(self.uid, -1, textureUniqueId=texUid)\n",
        "\n",
        "class Sphere(RigidBody):\n",
        "  \"\"\" We use this class, which inherits from the RigidBody class, to create the\n",
        "  hands and face of our character.\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "      transform,\n",
        "      color = np.array([0, 1, 1, 1]),\n",
        "      radius = 1.0):\n",
        "    super().__init__(transform)\n",
        "    self.radius = radius\n",
        "\n",
        "    self.visualId = p.createVisualShape(p.GEOM_SPHERE, radius=radius, rgbaColor=color, specularColor=[1, 1, 1])\n",
        "    position, orientation = self.get_position_and_orientation()\n",
        "    self.uid = p.createMultiBody(1.0, -1, self.visualId, position, orientation)\n",
        "\n",
        "\n",
        "def reset_simulation():\n",
        "  global body_list\n",
        "  time = 0\n",
        "  p.resetSimulation()\n",
        "  p.setGravity(0, 0, 0)\n",
        "  p.setTimeStep(1e-6) # Dummy time step\n",
        "  body_list = list()\n",
        "\n",
        "\n",
        "def create_rigid_bodies():\n",
        "  global name_to_transform, target_body_list\n",
        "  black_rgba = [0, 0, 0, 0.64]\n",
        "  white_rgba = [1, 1, 1, 0.64]\n",
        "  yellow_rgba = [1, 1, 0, 0.64]\n",
        "  # texUid = p.loadTexture(\"AK7aPaDMGbPI4WXfr9GBmMOGPiuTmOkomDPeXVKgrXzefHIiFpOQuXO8ZY1j3TG8aipbi067Q5WFqj4yn1WmS2SkJYGcQTeIww=s2560\")\n",
        "  # Box(name_to_transform['base'], half_extends=[0.2, 0.1, 0.3], color=white_rgba, texUid=texUid)\n",
        "  Box(name_to_transform['base'], half_extends=[0.2, 0.1, 0.3], color=white_rgba)\n",
        "\n",
        "  Sphere(name_to_transform['head'], radius=0.15, color=yellow_rgba)\n",
        "  for d in ['l', 'r']:\n",
        "    Box(name_to_transform['%s_upperarm1' % d], half_extends=[0.14, 0.05, 0.05], color=yellow_rgba)\n",
        "    Box(name_to_transform['%s_lowerarm1' % d], half_extends=[0.14, 0.05, 0.05], color=white_rgba)\n",
        "    Sphere(name_to_transform['%s_hand' % d], radius=0.05, color=yellow_rgba)\n",
        "    Box(name_to_transform['%s_thigh1' % d], half_extends=[0.08, 0.06, 0.18], color=yellow_rgba)\n",
        "    Box(name_to_transform['%s_shin1' % d], half_extends=[0.08, 0.06, 0.18], color=black_rgba)\n",
        "    Box(name_to_transform['%s_feet' % d], half_extends=[0.05, 0.10, 0.03], color=yellow_rgba)\n",
        "\n",
        "  max_targets = 10\n",
        "  for i in range(max_targets):\n",
        "    visualId = p.createVisualShape(p.GEOM_SPHERE, radius=0.06, rgbaColor=[1.0, 0.0, 0.0, 1.0], specularColor=[1, 1, 1])\n",
        "    uid = p.createMultiBody(1.0, -1, visualId, [100.0, 100.0, 100.0], [0.0, 0.0, 0.0, 1.0])\n",
        "    target_body_list.append(uid)\n",
        "\n",
        "setup_character()\n",
        "reset_simulation()\n",
        "create_rigid_bodies()\n",
        "\n",
        "_, _, img, _, _ = p.getCameraImage(pixelWidth, pixelHeight, viewMatrix,projectionMatrix, shadow=1, lightDirection=[1, 1, 1])\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Z-68R1DVluT"
      },
      "source": [
        "We will now define some helper functions that will allow us to connect the pose vector and the skeleton `Transformation`s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmp8I7QdVqLd"
      },
      "outputs": [],
      "source": [
        "#@title Pose <-> Transformation helpers\n",
        "\n",
        "def set_joint_positions(q):\n",
        "  global transform_list_1dof\n",
        "  assert len(q) == len(transform_list_1dof)\n",
        "  for q_i, jnt_i in zip(q, transform_list_1dof):\n",
        "    jnt_i.set_dof_value(value=q_i)\n",
        "\n",
        "\n",
        "def get_joint_positions():\n",
        "  global transform_list_1dof\n",
        "  return np.array([jnt_i.get_dof_value() for jnt_i in transform_list_1dof])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPFb8XRuVyhe"
      },
      "source": [
        "# 3. Inverse Kinematics.\n",
        "Now that we have all the required machinery in place, we will work on the final piece of the puzzle, the IK solver.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfPXQ8CTRUS1"
      },
      "source": [
        "## 3.1 Jacobian function.\n",
        "First, please implement the `compute_jacobian()` function according to its docstring, referring to the slides: *08_character_animation.pdf, slide 51-55*.\n",
        "\n",
        "Specifically, given an end effector's `Transformation` (at a leaf of the skeleton hierarchy), this function should return the Jacobian of the constrained point (3D) on the end effector with respect to the skeleton's pose vector (i.e. the values of all its DOFs). So, its shape should be `(3, ndofs)`.\n",
        "\n",
        "The key idea is to use these quantities for each joint that the end effector depends on:\n",
        "1. The transformations up the kinematic chain, from the immediate parent to the root\n",
        "1. The local derivative with respective to the joint's own DOF value\n",
        "1. The transformations down the kinematic chain, from the joint itself to the end effector\n",
        "1. Finally, the position of the constrained point in the end effector's local coordinate space ($\\mathbf{h}_0$ in the slides)\n",
        "    - This one is trivial in our case: *we (arbitrarily) define the constrained point to be at the origin of this local space.*\n",
        "\n",
        "Some hints based on what the code has already computed for you so far:\n",
        "\n",
        "- `transform_list_1dof` is a global list of transformations in the skeleton that have 1 DOF.\n",
        "    - The Jacobian's columns correspond to the joints in this list only: no other joints are relevant to any Jacobian.\n",
        "    - This list has been computed for you when the skeleton was built inside `setup_character()`.\n",
        "\n",
        "- Each `Transformation` has a member variable `dependent_transformations`: a set of all `Transformation`s whose DOFs it depends on.\n",
        "    - So, for a joint `j`, any joint outside the set `j.dependent_transformations` will also be irrelevant to the Jacobian of `j`.\n",
        "    - This set has been computed for you for each `Transformation` in the skeleton, again inside `setup_character()`.\n",
        "\n",
        "**<font color='orange'> \\*\\* Task 3: write your code below (20 pts) \\*\\* </font>**\n",
        "\n",
        "**<font color='orange'> \\*\\* Note: The implementation of Task 3 is given. However, we recommend students to read the code carefully. \\*\\* </font>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PstnQ_SV1bl"
      },
      "outputs": [],
      "source": [
        "def compute_jacobian(end_effector):\n",
        "  \"\"\" Compute the Jacobian of the end effector. You will need to make use of the\n",
        "  global_transform(), local_derivative() and transform_between() functions.\n",
        "  Don't forget to consider the (trivial) case when the joint\n",
        "  is not in end_effector.dependent_transformations.\n",
        "  And also note that each joint has only one degree of freedom.\n",
        "\n",
        "  Args:\n",
        "    end_effector: component for which we want to find the Jacobian\n",
        "\n",
        "  Returns:\n",
        "    J: Jacobian matrix of shape (3, ndofs)\n",
        "  \"\"\"\n",
        "  global transform_list_1dof\n",
        "  ndofs = len(transform_list_1dof)\n",
        "  h0 = np.array([0.0, 0.0, 0.0, 1.0]) # Our local point in a homogenous coordiate\n",
        "  J = np.zeros((3, ndofs))\n",
        "  for i, jnt_i in enumerate(transform_list_1dof):\n",
        "    # Analytical\n",
        "    if jnt_i not in end_effector.dependent_transformations:\n",
        "      continue\n",
        "    # Step 0. Compute T_parent. We provide this value as example.\n",
        "    if jnt_i.parent:  # check if the joint has a parent (not None)\n",
        "      # T_parent is the global transformation of the parent node (jnt_i.parent).\n",
        "      T_parent = jnt_i.parent.global_transform()\n",
        "    else:\n",
        "      T_parent = np.identity(4)  # If the joint has no parent.\n",
        "    # Student code starts here\n",
        "    # Step 1. compute dTdq, which is the local derivative of the current joint\n",
        "    dTdq = jnt_i.local_derivative()\n",
        "    # Step 2. Compute T_child, which is a transformation between jnt_i and end_effector. Compute the transformation between jnt_i and end_effector.\n",
        "    T_child = transform_between(jnt_i, end_effector)\n",
        "    # Step 3. Compute dx_dq\n",
        "    # dx_dq = (T_parent @ dTdq @ T_child)[:3, 3]\n",
        "    dx_dq = T_parent @ dTdq @ T_child @ h0\n",
        "    # Student code ends here\n",
        "    # Because dx_dq is a 4-dim homogenous vector with the 1.0 at the end, we take the first three.\n",
        "    J[:, i] = dx_dq[:3]\n",
        "  return J"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6rELVf1V698"
      },
      "source": [
        "Let's test the Jacobian function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5Pm7tPORWDP_"
      },
      "outputs": [],
      "source": [
        "#@title `compute_jacobian()` unit tests\n",
        "\n",
        "ndofs = len(transform_list_1dof)\n",
        "eps = 0.0001\n",
        "q = np.random.rand(ndofs) - 0.5\n",
        "set_joint_positions(q)\n",
        "\n",
        "for end_effector_name in ['l_hand', 'r_feet', 'head']:\n",
        "  end_effector = name_to_transform[end_effector_name]\n",
        "  J = compute_jacobian(end_effector)\n",
        "  for i, jnt_i in enumerate(transform_list_1dof):\n",
        "    # Analytical\n",
        "    dx_dq = J[:, i]\n",
        "\n",
        "    # Approximation\n",
        "    jnt_i.set_dof_value(value=q[i] + eps)\n",
        "    x_p = end_effector.global_position()\n",
        "    jnt_i.set_dof_value(value=q[i] - eps)\n",
        "    x_m = end_effector.global_position()\n",
        "    dx_dq_approx = (x_p - x_m) / (2 * eps)\n",
        "    jnt_i.set_dof_value(value=q[i])\n",
        "\n",
        "    # # Uncomment these lines to help with debugging (they print a lot of info though)\n",
        "    # print('test Jacobian for joint %s' % str(jnt_i))\n",
        "    # print(dx_dq)\n",
        "\n",
        "    assert np.allclose(dx_dq, dx_dq_approx, atol=1e-6)\n",
        "  print(f'passed the test for end effector: {end_effector_name}')\n",
        "print('all pass!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sSLJu_hWFFE"
      },
      "source": [
        "## 3.2 Objective and gradient functions.\n",
        "Next, please implement the objective and gradient functions, according to the respective docstrings. The gradient is with respect to the pose of the skeleton, which is now being referred to as `x` in the code instead of `q`.\n",
        "\n",
        "**<font color='orange'> \\*\\* Task 4 and 5: write your code below (20 + 20 = 40 pts) \\*\\* </font>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZipWx3cCBOEN"
      },
      "outputs": [],
      "source": [
        "end_effectors = [name_to_transform['l_hand'], name_to_transform['r_feet']]\n",
        "targets = [np.array((1.2, -0.3, 0.4)), np.array((0.0, -0.2, 0.1))]\n",
        "\n",
        "\n",
        "def objective_function(x):\n",
        "  \"\"\" Implement the objective function here. You will need to implement\n",
        "  the objective function of the form 0.5 * L2_norm(error)^2, where the\n",
        "  error is the difference between each position and its target.\n",
        "  Don't forget to add the contributions of all the positions/targets.\n",
        "\n",
        "  Useful functions include global_position() and np.linalg.norm().\n",
        "  You can assume that there is no additional objective (g(q) = 0).\n",
        "\n",
        "  Args:\n",
        "    x: Pose\n",
        "\n",
        "  Returns:\n",
        "    ret: The scalar value of the objective function.\n",
        "  \"\"\"\n",
        "  global end_effectors, targets, motion\n",
        "  motion.append(np.array(x))\n",
        "  set_joint_positions(x) # Set the joint position: q=x.\n",
        "  ret = 0.0\n",
        "  for end_effector, target in zip(end_effectors, targets):\n",
        "    # Student code starts here\n",
        "    # Step 1. get the global position of the current end effector, h(q). You can call end_effector.global_position().\n",
        "    h_q = end_effector.global_position()\n",
        "\n",
        "    # Step 2. compute the error as |h(q) - p|^2, where p is the target.\n",
        "    error = h_q - target\n",
        "\n",
        "    # Step 3. add the error to the ret.\n",
        "    ret += 0.5 * np.linalg.norm(error)**2\n",
        "\n",
        "    # Student code ends here\n",
        "  return ret\n",
        "\n",
        "def gradient_function(x):\n",
        "  \"\"\" Implement the gradient of the objective function above. Useful\n",
        "  functions include global_position() and compute_jacobian().\n",
        "\n",
        "  Args:\n",
        "    x: Pose\n",
        "\n",
        "  Returns:\n",
        "    grad: The len(x)-sized gradient vector.\n",
        "  \"\"\"\n",
        "  global end_effectors, targets\n",
        "  set_joint_positions(x)\n",
        "  grad = np.zeros(len(x))\n",
        "  for end_effector, target in zip(end_effectors, targets):\n",
        "    # Student code starts here\n",
        "    # Step 1. get the position of the end effector h(q)\n",
        "    h_q = end_effector.global_position()\n",
        "\n",
        "    # Step 2. compute the jacobian J, using what you implemented (or given)\n",
        "    J = compute_jacobian(end_effector)\n",
        "\n",
        "    # Step 3. Compute the gradient as J^T C, where C = h(q) - p\n",
        "    error = h_q - target\n",
        "\n",
        "    # Step 4. Add the gradient to the grad\n",
        "    grad += J.T @ error\n",
        "\n",
        "    # Student code ends here\n",
        "  return grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-iSaeJHXT8j"
      },
      "source": [
        "We test the objective and gradient implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JCxMKbEAXXkL"
      },
      "outputs": [],
      "source": [
        "#@title Objective & Gradient unit tests\n",
        "\n",
        "for test_id in range(10):\n",
        "  x0 = -10.0 + 20.0 * np.random.rand(ndofs)\n",
        "  # Exact\n",
        "  g = gradient_function(x0)\n",
        "  # Approximation\n",
        "  g_approx = np.zeros(ndofs)\n",
        "  eps = 0.001\n",
        "  for i in range(ndofs):\n",
        "    delta = np.zeros(ndofs)\n",
        "    delta[i] = eps\n",
        "    f_p = objective_function(x0 + delta)\n",
        "    f_m = objective_function(x0 - delta)\n",
        "    g_approx[i] = (f_p - f_m) / (eps * 2.0)\n",
        "  print('test gradient against the random pose #%d' % test_id)\n",
        "  assert np.allclose(g, g_approx, atol=1e-6)\n",
        "print('all pass!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rt3K2J0p6Q4l"
      },
      "source": [
        "Let's now solve for the motion. The targets will be added sequentially, and each time a new target is added, we will solve the pose of the character and linearly interpolate in-between frames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "e4BNOMs_6SMw"
      },
      "outputs": [],
      "source": [
        "#@title Motion IK + interpolation (Twister) algorithm\n",
        "\n",
        "all_end_effectors = [name_to_transform['l_hand'], name_to_transform['r_feet'], name_to_transform['r_hand'], name_to_transform['l_feet'], name_to_transform['l_upperarm2']]\n",
        "all_targets = [np.array((0.6, -0.3, -0.3)), np.array((-0.2, 0.3, -1.0)), np.array((-0.5, 0.2, -0.3)), np.array((-0.5, -0.3, -0.7)), np.array((0.5, -0.3, 0.1))]\n",
        "end_effectors = list()\n",
        "targets = list()\n",
        "motion = list()\n",
        "for uid in target_body_list:\n",
        "  p.resetBasePositionAndOrientation(uid, [100.0, 100.0, 100.0], [0.0, 0.0, 0.0, 1.0])\n",
        "frame = 0\n",
        "\n",
        "q0 = np.zeros(ndofs)\n",
        "set_joint_positions(q0)\n",
        "\n",
        "for idx, (end_effector, target) in enumerate(zip(all_end_effectors, all_targets)):\n",
        "  p.resetBasePositionAndOrientation(target_body_list[idx], target, [0.0, 0.0, 0.0, 1.0])\n",
        "  print('add constraint for: %s' % (str(end_effector)))\n",
        "  # We will add a constraint one by one, and solve the corresponding pose\n",
        "  end_effectors.append(end_effector)\n",
        "  targets.append(target)\n",
        "\n",
        "  # Solve the pose\n",
        "  res = minimize(objective_function,\n",
        "                x0=np.zeros(ndofs),\n",
        "                jac=gradient_function,  # Hint: you can comment out this line to test your objective function. gradient will be computed numerically.\n",
        "                method='SLSQP',\n",
        "                options={'ftol': 1e-8, 'disp': True})\n",
        "  print(res)\n",
        "\n",
        "  if not os.path.exists(\"./frames/\"):\n",
        "    os.makedirs(\"./frames/\")\n",
        "\n",
        "  q1 = res['x']\n",
        "  for w in np.linspace(0.0, 2.0, 41):\n",
        "    w = min(w, 1.0)\n",
        "    q = (1 - w) * q0 + w * q1\n",
        "    set_joint_positions(q)\n",
        "    for rb in body_list:\n",
        "      rb.update_pybullet()\n",
        "    # Render frame\n",
        "    _, _, img, _, _ = p.getCameraImage(pixelWidth, pixelHeight, viewMatrix,projectionMatrix, shadow=1, lightDirection=[1,1,1])\n",
        "    Image.fromarray(img[:, :, :3]).save('./frames/frame%04d.jpg' % frame)\n",
        "    frame += 1\n",
        "    if frame % 10 == 0:\n",
        "      print(\"Rendering frame #%d\" % frame)\n",
        "  q0 = q1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l14v4xHmZGMX"
      },
      "source": [
        "Create a video of the motion we just solved for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XF0Ppk-_ZHK7"
      },
      "outputs": [],
      "source": [
        "#@title Motion IK + interpolation (Twister) video\n",
        "\n",
        "ffmpeg.input('./frames/frame*.jpg', pattern_type='glob', framerate=20).output('./output.gif').overwrite_output().run()\n",
        "video = io.open('./output.gif', 'r+b').read()\n",
        "encoded = base64.b64encode(video)\n",
        "ipythondisplay.display(HTML(data='''<img src=\"data:image/gif;base64,{0}\"/>'''.format(encoded.decode('ascii'))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkAV36EO8OSM"
      },
      "source": [
        "### FK vs. IK\n",
        "\n",
        "Can you see the animation of the character who is playing our version of the Twister game, as dictated by the targets?\n",
        "\n",
        "You can also check the quality of IK by looking at the `fun` value of the optimization results `res`, printed by the cell containing the Twister algorithm. If it is reasonably small (< 1e-4), we can call it a success.\n",
        "\n",
        "Imagine if you want to describe a similar motion using forward kinematics, and discuss the benefit of inverse kinematics.\n",
        "\n",
        "**<font color='orange'> \\*\\*write your answer below (20 pts)\\*\\* </font>**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4Vx6RezsCUh"
      },
      "source": [
        "If we used forward kinematics to describe similar motions, we would need to manually find the joint rotations that would result in the body parts ending up at the desired points, and explicitly provide them to the FK position solver.\n",
        "\n",
        "We would need to do this for every frame, and we would be responsible for making sure all the joint configurations together are physically plausible each step of the way.\n",
        "In practice, this would involve many rounds of trial error, and would likely result in less realistic poses, since FK does not automatically update all joints positions.\n",
        "\n",
        "Inverse kinematics solves precisely these issues by allowing us to specify the _end positions_ we want and solving for all joint angles and intermediate positions based on the current position constraints _automatically_.\n",
        "\n",
        "This allows for much more intuitive, goal-oriented control, results in more natural poses, and also makes it easy to incorporate environmental constraints - all while being much less labor intensive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaKy7gSz91sK"
      },
      "source": [
        "## 3.3. Motion Capture (Mocap)\n",
        "In this section, we visualize the motion capture data obtained from the mocap session week.\n",
        "\n",
        "\n",
        "We then use inverse kinematics we implemented to retarget our robot to the human motion capture.\n",
        "\n",
        "**There's no graded points in this section.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLp9RwIZRUS5"
      },
      "source": [
        "### 3.3.1 Download Motion Capture Data\n",
        "First, let's download the motion capture data, captured from mocap-room. Running this code will save the data in \"mocap_data\" folder. You can also take a look at the gifs from: https://drive.google.com/drive/folders/1IUbXcWGuujtOuAefxct24GvQwYiUE_9X\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgt-dZhB900I"
      },
      "outputs": [],
      "source": [
        "#@title Download mocap data\n",
        "!pip install gdown\n",
        "# download motion capture files from google drive\n",
        "!gdown https://drive.google.com/drive/folders/1IUbXcWGuujtOuAefxct24GvQwYiUE_9X -O mocap_data --folder --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bj2GMPLwBcm7"
      },
      "outputs": [],
      "source": [
        "#@title Load mocap data from file <br> *(un-comment your favorite motion capture data along with corresponding scale and view matrix)*\n",
        "\n",
        "mocap_filename = \"mocap_data/angela.npy\"\n",
        "mocap_frame_scale = 1.2/1000\n",
        "viewMatrix_mocap = p.computeViewMatrixFromYawPitchRoll(cameraTargetPosition=[0,0, 0.8], distance=3.5, yaw=180.0, pitch=-1.0, roll=0.0, upAxisIndex=2)\n",
        "\n",
        "# mocap_filename = \"mocap_data/avery.npy\"\n",
        "# mocap_frame_scale = 1.2/1000\n",
        "# viewMatrix_mocap = p.computeViewMatrixFromYawPitchRoll(cameraTargetPosition=[0,0, 0.8], distance=3.5, yaw=180.0, pitch=-1.0, roll=0.0, upAxisIndex=2)\n",
        "\n",
        "# mocap_filename = \"mocap_data/david.npy\"\n",
        "# mocap_frame_scale = 1.2/1000\n",
        "# viewMatrix_mocap = p.computeViewMatrixFromYawPitchRoll(cameraTargetPosition=[0,0, 0.8], distance=3.5, yaw=90.0, pitch=-1.0, roll=0.0, upAxisIndex=2)\n",
        "\n",
        "# mocap_filename = \"mocap_data/actor_unknown.npy\" # Forgot the name of Monday 2nd section, 2nd actor.\n",
        "# mocap_frame_scale = 1.2/1000\n",
        "# viewMatrix_mocap = p.computeViewMatrixFromYawPitchRoll(cameraTargetPosition=[0,0, 0.8], distance=3.5, yaw=90.0, pitch=-1.0, roll=0.0, upAxisIndex=2)\n",
        "\n",
        "# mocap_filename = \"mocap_data/jack.npy\"\n",
        "# mocap_frame_scale = 1.2/1000\n",
        "# viewMatrix_mocap = p.computeViewMatrixFromYawPitchRoll(cameraTargetPosition=[0,0, 0.8], distance=3.5, yaw=180.0, pitch=-1.0, roll=0.0, upAxisIndex=2)\n",
        "\n",
        "# mocap_filename = \"mocap_data/jade.npy\"\n",
        "# mocap_frame_scale = 1.2/1000\n",
        "# viewMatrix_mocap = p.computeViewMatrixFromYawPitchRoll(cameraTargetPosition=[0,0, 0.8], distance=3.5, yaw=180.0, pitch=-1.0, roll=0.0, upAxisIndex=2)\n",
        "\n",
        "# mocap_filename = \"mocap_data/john.npy\"\n",
        "# mocap_frame_scale = 1.2/1000\n",
        "# viewMatrix_mocap = p.computeViewMatrixFromYawPitchRoll(cameraTargetPosition=[0,0, 0.8], distance=3.5, yaw=180.0, pitch=-1.0, roll=0.0, upAxisIndex=2)\n",
        "\n",
        "# mocap_filename = \"mocap_data/nicole.npy\"  # one marker was detached after 6 secs\n",
        "# mocap_frame_scale = 1.2/1000\n",
        "# viewMatrix_mocap = p.computeViewMatrixFromYawPitchRoll(cameraTargetPosition=[0,0, 0.8], distance=3.5, yaw=180.0, pitch=-1.0, roll=0.0, upAxisIndex=2)\n",
        "\n",
        "# mocap_filename = \"mocap_data/rahul.npy\"\n",
        "# mocap_frame_scale = 1.2/1000\n",
        "# viewMatrix_mocap = p.computeViewMatrixFromYawPitchRoll(cameraTargetPosition=[0,0, 0.8], distance=3.5, yaw=180.0, pitch=-1.0, roll=0.0, upAxisIndex=2)\n",
        "\n",
        "# mocap_filename = \"mocap_data/seyeon.npy\"\n",
        "# mocap_frame_scale = 1.2/1000\n",
        "# viewMatrix_mocap = p.computeViewMatrixFromYawPitchRoll(cameraTargetPosition=[0,0, 0.8], distance=3.5, yaw=180.0, pitch=-1.0, roll=0.0, upAxisIndex=2)\n",
        "\n",
        "# mocap_filename = \"mocap_data/william.npy\"   # one marker was detached from the beginning\n",
        "# mocap_frame_scale = 1.2/1000\n",
        "# viewMatrix_mocap = p.computeViewMatrixFromYawPitchRoll(cameraTargetPosition=[0,0, 0.8], distance=3.5, yaw=180.0, pitch=-1.0, roll=0.0, upAxisIndex=2)\n",
        "\n",
        "\n",
        "# mocap_filename = \"mocap_data/xinwen.npy\"\n",
        "# mocap_frame_scale = 1.2/1000\n",
        "# viewMatrix_mocap = p.computeViewMatrixFromYawPitchRoll(cameraTargetPosition=[0,0, 0.8], distance=3.5, yaw=90.0, pitch=-1.0, roll=0.0, upAxisIndex=2)\n",
        "\n",
        "\n",
        "\n",
        "mocap_frames = np.load(mocap_filename) # data in shape (n_frame, 6, 4) array, where 6 means number of end effector, 4 is (x,y,z, occluded_flag)\n",
        "mocap_frame_rep = ['_Spine', '_RightFoot', '_RightHand', '_LeftFoot', '_Head', '_LeftHand']  # order of end_effector inside data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShLd_FCUCe8B"
      },
      "source": [
        "Our data is recorded in 120Hz, meaning each frame is 8.3 miliseconds apart.\n",
        "\n",
        "We reduce the size of our data by taking only 2 second of entire frame starting from 600th frame. We also reduce the framerate to 30Hz by taking only every four frames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5Gnr15uCd_O"
      },
      "outputs": [],
      "source": [
        "#@title Frame pre-processing\n",
        "\n",
        "# First, make all occluded frame become previous non-occluded frame\n",
        "for f_idx in range(mocap_frames.shape[0]):\n",
        "  mocap_frame = mocap_frames[f_idx]\n",
        "  for ee_idx in range(mocap_frame.shape[0]):\n",
        "    ee_occluded = mocap_frame[ee_idx, 3]\n",
        "    if ee_occluded and f_idx > 0:\n",
        "      mocap_frames[f_idx, ee_idx, :3] = mocap_frames[f_idx-1, ee_idx, :3]\n",
        "\n",
        "\n",
        "start_frame_idx = 0\n",
        "end_frame_idx = 1200\n",
        "total_render_frames = (end_frame_idx - start_frame_idx) // 4\n",
        "\n",
        "# Our data is recorded 120Hz, meaning each frame is 8.3 milisecond apart. We only take 1/4 of the data, making it 30Hz\n",
        "target_mocap_frames = mocap_frames[start_frame_idx:end_frame_idx:4, :, :3]\n",
        "print(target_mocap_frames.shape)\n",
        "\n",
        "# normalize x,y axis\n",
        "target_mocap_frames_mean = np.mean(target_mocap_frames, axis=(0,1))\n",
        "target_mocap_frames_mean[2] = 0\n",
        "\n",
        "# mocap_frame_scale = 1.2/1000  # scale to match the size of our object.\n",
        "# mocap_frame_scale = 1.1/1000  # scale to match the size of our object.\n",
        "\n",
        "target_mocap_frames = (target_mocap_frames - target_mocap_frames_mean) * mocap_frame_scale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KbqWz49Fnqx"
      },
      "source": [
        "Visualize our Mocap Data to check it is valid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "c3hEGnAMB3pD"
      },
      "outputs": [],
      "source": [
        "#@title Mocap visualization & validation\n",
        "\n",
        "from matplotlib import animation\n",
        "# Do 3D plot of data\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(projection=\"3d\")\n",
        "\n",
        "ax.set(xlim3d=(-1,1), xlabel='X')\n",
        "ax.set(ylim3d=(-1, 1), ylabel='Y')\n",
        "ax.set(zlim3d=(-0.5, 1.5), zlabel='Z')\n",
        "ax.view_init(10, 70)\n",
        "\n",
        "rods = [_] * 10\n",
        "rods[0], = ax.plot([], [], 'ro-') # spline -> Head\n",
        "rods[1], = ax.plot([], [], 'ko-') # spline -> Right Foot\n",
        "rods[2], = ax.plot([], [], 'ko-') # spline -> Right Hand\n",
        "rods[3], = ax.plot([], [], 'ko-') # spline -> Left Foot\n",
        "rods[4], = ax.plot([], [], 'ko-') # spline -> Left Hand\n",
        "\n",
        "idx_spline = 0\n",
        "idx_rightfoot = 1\n",
        "idx_righthand = 2\n",
        "idx_leftfoot = 3\n",
        "idx_head = 4\n",
        "idx_lefthand = 5\n",
        "\n",
        "x=0\n",
        "y=1\n",
        "z=2\n",
        "\n",
        "def plot_scene(f_idx):\n",
        "  frame = target_mocap_frames[f_idx]\n",
        "\n",
        "  rods[0].set_data([frame[idx_spline, x], frame[idx_head,x]], [frame[idx_spline, y], frame[idx_head, y]])\n",
        "  # NOTE: there is no .set_data() for 3 dim data...\n",
        "  rods[0].set_3d_properties([frame[idx_spline, z], frame[idx_head,z]])\n",
        "\n",
        "  rods[1].set_data([frame[idx_spline, x], frame[idx_rightfoot,x]], [frame[idx_spline, y], frame[idx_rightfoot, y]])\n",
        "  rods[1].set_3d_properties([frame[idx_spline, z], frame[idx_rightfoot,z]])\n",
        "\n",
        "  rods[2].set_data([frame[idx_spline, x], frame[idx_righthand,x]], [frame[idx_spline, y], frame[idx_righthand, y]])\n",
        "  rods[2].set_3d_properties([frame[idx_spline, z], frame[idx_righthand,z]])\n",
        "\n",
        "  rods[3].set_data([frame[idx_spline, x], frame[idx_leftfoot,x]], [frame[idx_spline, y], frame[idx_leftfoot, y]])\n",
        "  rods[3].set_3d_properties([frame[idx_spline, z], frame[idx_leftfoot,z]])\n",
        "\n",
        "  rods[4].set_data([frame[idx_spline, x], frame[idx_lefthand,x]], [frame[idx_spline, y], frame[idx_lefthand, y]])\n",
        "  rods[4].set_3d_properties([frame[idx_spline, z], frame[idx_lefthand,z]])\n",
        "\n",
        "# Called before the first frame\n",
        "def init_animation():\n",
        "  plot_scene(0)\n",
        "  return []\n",
        "\n",
        "\n",
        "# Called every frame\n",
        "def animate(i):\n",
        "  if i % 10 == 0:\n",
        "      print(\"Generating frame #%d...\" % i)\n",
        "  plot_scene(i)\n",
        "  return []\n",
        "\n",
        "anim = animation.FuncAnimation(fig, animate, init_func=init_animation, frames=total_render_frames, interval=33, blit=False) # 30Hz\n",
        "# HTML(anim.to_html5_video())\n",
        "HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hC67T204Fupr"
      },
      "source": [
        "### 3.3.2 Retarget our Motion Capture to our Figure using Inverse Kinematics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvMqyJKqF5Sn"
      },
      "outputs": [],
      "source": [
        "#@title Retargeting mocap to PyBullet via IK: Algorithm\n",
        "\n",
        "# Add plane\n",
        "import pybullet_data as pd\n",
        "p.setAdditionalSearchPath(pd.getDataPath())\n",
        "planeId = p.loadURDF(\"plane.urdf\")\n",
        "\n",
        "# You can use different viewMatrix depending on your mocap data for better visualization\n",
        "# Rotate Yaw 180 degrees to make \"left\" to be +x direction as in our mocap-sytstem. You can modify this line for better view of the character\n",
        "# viewMatrix_mocap = p.computeViewMatrixFromYawPitchRoll(cameraTargetPosition=[0,0, 0.8], distance=3.5, yaw=90.0, pitch=-1.0, roll=0.0, upAxisIndex=2)\n",
        "\n",
        "\n",
        "all_end_effectors = [name_to_transform['r_feet'], name_to_transform['r_hand'], name_to_transform['l_feet'], name_to_transform['head'], name_to_transform['l_hand']]\n",
        "\n",
        "end_effectors = list()\n",
        "targets = list()\n",
        "motion = list()\n",
        "for uid in target_body_list:\n",
        "  p.resetBasePositionAndOrientation(uid, [100.0, 100.0, 100.0], [0.0, 0.0, 0.0, 1.0])\n",
        "frame = 0\n",
        "\n",
        "q0 = np.zeros(ndofs)\n",
        "set_joint_positions(q0)\n",
        "\n",
        "q = q0\n",
        "duration_list = []\n",
        "# for idx in range(60): # don't print all frames..\n",
        "for idx in range(300): # don't print all frames..\n",
        "  frame = target_mocap_frames[idx]\n",
        "\n",
        "\n",
        "  end_effectors = all_end_effectors\n",
        "  targets = [frame[idx_rightfoot, :3], frame[idx_righthand, :3], frame[idx_leftfoot, :3], frame[idx_head, :3], frame[idx_lefthand, :3]]\n",
        "  for t_idx, target in enumerate(targets):\n",
        "    p.resetBasePositionAndOrientation(target_body_list[t_idx], target, [0.0, 0.0, 0.0, 1.0])\n",
        "\n",
        "  # Solve the pose\n",
        "  res = minimize(objective_function,\n",
        "                x0=q, # Start from previous pose\n",
        "                jac=gradient_function,  # Hint: you can comment out this line to test your objective function. gradient will be computed numerically.\n",
        "                method='SLSQP',\n",
        "                options={'ftol': 1e-4, 'disp': False})  # increase ftol for faster rendering\n",
        "                # options={'ftol': 1e-8, 'disp': False})\n",
        "  # print(res)\n",
        "  q = res['x']\n",
        "  set_joint_positions(q)\n",
        "  for rb in body_list:\n",
        "    rb.update_pybullet()\n",
        "  # Render frame\n",
        "  _, _, img, _, _ = p.getCameraImage(pixelWidth, pixelHeight, viewMatrix_mocap,projectionMatrix, shadow=1, lightDirection=[1,1,1])\n",
        "  Image.fromarray(img[:, :, :3]).save('./frames/frame_mocap%04d.jpg' % idx)\n",
        "  if idx % 10 == 0:\n",
        "    print(\"Rendering frame #%d\" % idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bl8jI08MGROh"
      },
      "outputs": [],
      "source": [
        "#@title Retargeting mocap to PyBullet via IK: Video\n",
        "\n",
        "ffmpeg.input('./frames/frame_mocap*.jpg', pattern_type='glob', framerate=33).output('./output_mocap.gif').overwrite_output().run()\n",
        "video = io.open('./output_mocap.gif', 'r+b').read()\n",
        "encoded = base64.b64encode(video)\n",
        "ipythondisplay.display(HTML(data='''<img src=\"data:image/gif;base64,{0}\"/>'''.format(encoded.decode('ascii'))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsbS3Zoh9x8b"
      },
      "source": [
        "# 4. Extra Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-S0d3hRmnnL"
      },
      "source": [
        "## 4.1 Discussing Motion Capture Result (New?)\n",
        "\n",
        "For some motion capture data, the retargeted motion doesn't resemble the actual human actor. Discuss what are the causes of it and what are some ways to make the motion of our robot to resemble that of human actor.\n",
        "\n",
        "\n",
        "**<font color='orange'> \\*\\*write your answer below (2 pts)\\*\\* </font>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHd2W6OImnHn"
      },
      "source": [
        "Our inverse kinematics solver enforces reasonably realistic joint angles throughout the animation by disallowing dramatic violations of the basic degrees of freedom of the joints in the human body.\n",
        "However, to further ensure the retargeted motion resembles real human movements, we would need to implement additional custom constraints into our solver system.\n",
        "\n",
        "These additional constraints could take the form of explicit heuristics to nudge the joint angles and positions towards more natural-looking movements, prevent physically impossible orientations, or enforce extra physical constraints such as energy minimization.\n",
        "We could also use data-driven machine learning approaches to align the movements with recorded human movements.\n",
        "\n",
        "In addition to the algorithmic limitations, there are also limitations to the motion capture technology and setting we used.\n",
        "For example, only five points on the body are tracked, requiring e.g. the knee and elbow positions to be inferred algorithmically.\n",
        "There are also limitations to the spatiotemporal granularity of the captured positions, as well as potential errors in estimating the true 3D positions of the emitters from the joint 2D projections recorded by cameras.\n",
        "\n",
        "Finally, we made no effort to incorporate hard constraints from the physical environment, such as ensuring the ground cannot be penetrated.\n",
        "\n",
        "Here are some approaches we could take to better align the motion with that of a human:\n",
        "- Add more (passive) markers and/or (active) emitters to the actor during motion capture.\n",
        "- Add more cameras at different positions/angles relative to the performer.\n",
        "- Incorporate other recorded optical data to more accurately infer the poses using machine-learning/data-driven methods.\n",
        "- Incorporate additional heuristic, environment, and/or physics-informed constraints into the IK solver."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErPid_d4-rF0"
      },
      "source": [
        "## 4.2. Memoization of `transform_between()`\n",
        "\n",
        "The `transform_between()` function is one of the busiest functions in the code. First, count the number of times it is called during one evaluation of the objective function.\n",
        "\n",
        "By caching the computation results, you may be able to save some function calls, which is called \"*memoization*\" (https://en.wikipedia.org/wiki/Memoization). Implement this caching mechanism and see if you can reduce the number of `transform_between()` function calls per objective evaluation.\n",
        "\n",
        "**Note:** *don't forget to erase the cache when we change the pose.*\n",
        "\n",
        "**<font color='orange'> \\*\\*edit the above code directly and explain your implementation & results - number of function calls - below (4 pts)\\*\\* </font>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg5pkcp8AFfJ"
      },
      "source": [
        "(explain your answer here)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVcDTu0wAHow"
      },
      "source": [
        "## 4.3. Never Leave The Targets\n",
        "\n",
        "At the final stage of the Twister video (after adding the final constraint), some of the character's end-effectors may leave the targets temporarily during the interpolation of pose. Refer to the frame below as an example *(if you don't see something like it in the Twister video for some reason, please give it harder target configurations)*.\n",
        "\n",
        "This is because we first solve the constrained IK problem once per stage (as each target is added), and then interpolate the motion freely in the pose space between the poses for each stage. Nothing forces the poses generated *during the interpolation* to satisfy all constraints that the targets specify.\n",
        "\n",
        "Please update the motion interpolation algorithm to prevent this behavior.\n",
        "\n",
        "**Hint:** *you may need to interpolate in the space of the targets instead, and solve the resulting IK problems more often.*\n",
        "\n",
        "**<font color='orange'> \\*\\*edit the above code directly and explain your implementation below (4 pts)\\*\\* </font>**\n",
        "\n",
        "![Bad Frame](http://cc.gatech.edu/~sha9/public/P5-twister-bad.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMzm-c7WB9w9"
      },
      "source": [
        "(explain your answer here)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "2e5314a4d3bbd0f86247ba18d17c0766895c8e7c17c397ab915e526c9228afab"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
