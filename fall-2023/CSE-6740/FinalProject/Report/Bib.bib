@inproceedings{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	volume = {25},
	url = {https://proceedings.neurips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
	urldate = {2023-12-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year = {2012},
	file = {Full Text PDF:/Users/khiner/Zotero/storage/4QJV6CPZ/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf:application/pdf},
}

@misc{graves_speech_2013,
	title = {Speech {Recognition} with {Deep} {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1303.5778},
	abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates {\textbackslash}emph\{deep recurrent neural networks\}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7\% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
	urldate = {2023-12-07},
	publisher = {arXiv},
	author = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
	month = mar,
	year = {2013},
	note = {arXiv:1303.5778 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: To appear in ICASSP 2013},
}
@inproceedings{piczak_environmental_2015,
	title = {Environmental sound classification with convolutional neural networks},
	url = {https://ieeexplore.ieee.org/document/7324337},
	doi = {10.1109/MLSP.2015.7324337},
	abstract = {This paper evaluates the potential of convolutional neural networks in classifying short audio clips of environmental sounds. A deep model consisting of 2 convolutional layers with max-pooling and 2 fully connected layers is trained on a low level representation of audio data (segmented spectrograms) with deltas. The accuracy of the network is evaluated on 3 public datasets of environmental and urban recordings. The model outperforms baseline implementations relying on mel-frequency cepstral coefficients and achieves results comparable to other state-of-the-art approaches.},
	urldate = {2023-12-07},
	booktitle = {2015 {IEEE} 25th {International} {Workshop} on {Machine} {Learning} for {Signal} {Processing} ({MLSP})},
	author = {Piczak, Karol J.},
	month = sep,
	year = {2015},
	note = {ISSN: 2378-928X},
	pages = {1--6},
}
@article{stowell_detection_2015,
	title = {Detection and {Classification} of {Acoustic} {Scenes} and {Events}},
	volume = {17},
	issn = {1520-9210, 1941-0077},
	url = {http://ieeexplore.ieee.org/document/7100934/},
	doi = {10.1109/TMM.2015.2428998},
	abstract = {Deep neural networks (DNNs) have recently achieved a great success in various learning task, and have also been used for classiﬁcation of environmental sounds. While DNNs are showing their potential in the classiﬁcation task, they cannot fully utilize the temporal information. In this paper, we propose a neural network architecture for the purpose of using sequential information. The proposed structure is composed of two separated lower networks and one upper network. We refer to these as LSTM layers, CNN layers and connected layers, respectively. The LSTM layers extract the sequential information from consecutive audio features. The CNN layers learn the spectro-temporal locality from spectrogram images. Finally, the connected layers summarize the outputs of two networks to take advantage of the complementary features of the LSTM and CNN by combining them. To compare the proposed method with other neural networks, we conducted a number of experiments on the TUT acoustic scenes 2016 dataset which consists of recordings from various acoustic scenes. By using the proposed combination structure, we achieved higher performance compared to the conventional DNN, CNN and LSTM architecture.},
	language = {en},
	number = {10},
	urldate = {2023-12-07},
	journal = {IEEE Transactions on Multimedia},
	author = {Stowell, Dan and Giannoulis, Dimitrios and Benetos, Emmanouil and Lagrange, Mathieu and Plumbley, Mark D.},
	month = oct,
	year = {2015},
	pages = {1733--1746},
	file = {Stowell et al. - 2015 - Detection and Classification of Acoustic Scenes an.pdf:/Users/khiner/Zotero/storage/NZ4GWK3T/Stowell et al. - 2015 - Detection and Classification of Acoustic Scenes an.pdf:application/pdf},
}
@misc{choi_convolutional_2016,
	title = {Convolutional {Recurrent} {Neural} {Networks} for {Music} {Classification}},
	url = {http://arxiv.org/abs/1609.04243},
	abstract = {We introduce a convolutional recurrent neural network (CRNN) for music tagging. CRNNs take advantage of convolutional neural networks (CNNs) for local feature extraction and recurrent neural networks for temporal summarisation of the extracted features. We compare CRNN with three CNN structures that have been used for music tagging while controlling the number of parameters with respect to their performance and training time per sample. Overall, we found that CRNNs show a strong performance with respect to the number of parameter and training time, indicating the effectiveness of its hybrid structure in music feature extraction and feature summarisation.},
	urldate = {2023-12-07},
	publisher = {arXiv},
	author = {Choi, Keunwoo and Fazekas, George and Sandler, Mark and Cho, Kyunghyun},
	month = dec,
	year = {2016},
	note = {arXiv:1609.04243 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Multimedia, Computer Science - Neural and Evolutionary Computing, Computer Science - Sound},
	annote = {Comment: 5 pages, ICASSP 2017 submitted. Revised to fix previous CNN architectures and update experiment results},
}
@miss{hershey_cnn_2017,
	title = {{CNN} {Architectures} for {Large}-{Scale} {Audio} {Classification}},
	url = {http://arxiv.org/abs/1609.09430},
	abstract = {Convolutional Neural Networks (CNNs) have proven very effective in image classification and show promise for audio. We use various CNN architectures to classify the soundtracks of a dataset of 70M training videos (5.24 million hours) with 30,871 video-level labels. We examine fully connected Deep Neural Networks (DNNs), AlexNet [1], VGG [2], Inception [3], and ResNet [4]. We investigate varying the size of both training set and label vocabulary, finding that analogs of the CNNs used in image classification do well on our audio classification task, and larger training and label sets help up to a point. A model using embeddings from these classifiers does much better than raw features on the Audio Set [5] Acoustic Event Detection (AED) classification task.},
	urldate = {2023-12-07},
	publisher = {arXiv},
	author = {Hershey, Shawn and Chaudhuri, Sourish and Ellis, Daniel P. W. and Gemmeke, Jort F. and Jansen, Aren and Moore, R. Channing and Plakal, Manoj and Platt, Devin and Saurous, Rif A. and Seybold, Bryan and Slaney, Malcolm and Weiss, Ron J. and Wilson, Kevin},
	month = jan,
	year = {2017},
	note = {arXiv:1609.09430 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Statistics - Machine Learning},
	annote = {Comment: Accepted for publication at ICASSP 2017 Changes: Added definitions of mAP, AUC, and d-prime. Updated mAP/AUC/d-prime numbers for Audio Set based on changes of latest Audio Set revision. Changed wording to fit 4 page limit with new additions},
}

@misc{chen_beats_2022,
	title = {{BEATs}: {Audio} {Pre}-{Training} with {Acoustic} {Tokenizers}},
	shorttitle = {{BEATs}},
	url = {http://arxiv.org/abs/2212.09058},
	abstract = {The massive growth of self-supervised learning (SSL) has been witnessed in language, vision, speech, and audio domains over the past few years. While discrete label prediction is widely adopted for other modalities, the state-of-the-art audio SSL models still employ reconstruction loss for pre-training. Compared with reconstruction loss, semantic-rich discrete label prediction encourages the SSL model to abstract the high-level audio semantics and discard the redundant details as in human perception. However, a semantic-rich acoustic tokenizer for general audio pre-training is usually not straightforward to obtain, due to the continuous property of audio and unavailable phoneme sequences like speech. To tackle this challenge, we propose BEATs, an iterative audio pre-training framework to learn Bidirectional Encoder representation from Audio Transformers, where an acoustic tokenizer and an audio SSL model are optimized by iterations. In the first iteration, we use random projection as the acoustic tokenizer to train an audio SSL model in a mask and label prediction manner. Then, we train an acoustic tokenizer for the next iteration by distilling the semantic knowledge from the pre-trained or fine-tuned audio SSL model. The iteration is repeated with the hope of mutual promotion of the acoustic tokenizer and audio SSL model. The experimental results demonstrate our acoustic tokenizers can generate discrete labels with rich audio semantics and our audio SSL models achieve state-of-the-art results across various audio classification benchmarks, even outperforming previous models that use more training data and model parameters significantly. Specifically, we set a new state-of-the-art mAP 50.6\% on AudioSet-2M for audio-only models without using any external data, and 98.1\% accuracy on ESC-50. The code and pre-trained models are available at https://aka.ms/beats.},
	urldate = {2023-12-07},
	publisher = {arXiv},
	author = {Chen, Sanyuan and Wu, Yu and Wang, Chengyi and Liu, Shujie and Tompkins, Daniel and Chen, Zhuo and Wei, Furu},
	month = dec,
	year = {2022},
	note = {arXiv:2212.09058 [cs, eess]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv.org Snapshot:/Users/khiner/Zotero/storage/B4RCM6XL/2212.html:text/html},
}
@misc{baevski_wav2vec_2020,
	title = {wav2vec 2.0: {A} {Framework} for {Self}-{Supervised} {Learning} of {Speech} {Representations}},
	shorttitle = {wav2vec 2.0},
	url = {http://arxiv.org/abs/2006.11477},
	doi = {10.48550/arXiv.2006.11477},
	abstract = {We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.},
	urldate = {2023-12-07},
	publisher = {arXiv},
	author = {Baevski, Alexei and Zhou, Henry and Mohamed, Abdelrahman and Auli, Michael},
	month = oct,
	year = {2020},
	note = {arXiv:2006.11477 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/Users/khiner/Zotero/storage/MPUCDC5G/Baevski et al. - 2020 - wav2vec 2.0 A Framework for Self-Supervised Learn.pdf:application/pdf},
}
@article{chen_wavlm_2022,
	title = {{WavLM}: {Large}-{Scale} {Self}-{Supervised} {Pre}-{Training} for {Full} {Stack} {Speech} {Processing}},
	volume = {16},
	issn = {1932-4553, 1941-0484},
	shorttitle = {{WavLM}},
	url = {http://arxiv.org/abs/2110.13900},
	doi = {10.1109/JSTSP.2022.3188113},
	abstract = {Self-supervised learning (SSL) achieves great success in speech recognition, while limited exploration has been attempted for other speech processing tasks. As speech signal contains multi-faceted information including speaker identity, paralinguistics, spoken content, etc., learning universal representations for all speech tasks is challenging. To tackle the problem, we propose a new pre-trained model, WavLM, to solve full-stack downstream speech tasks. WavLM jointly learns masked speech prediction and denoising in pre-training. By this means, WavLM does not only keep the speech content modeling capability by the masked speech prediction, but also improves the potential to non-ASR tasks by the speech denoising. In addition, WavLM employs gated relative position bias for the Transformer structure to better capture the sequence ordering of input speech. We also scale up the training dataset from 60k hours to 94k hours. WavLM Large achieves state-of-the-art performance on the SUPERB benchmark, and brings significant improvements for various speech processing tasks on their representative benchmarks. The code and pre-trained models are available at https://aka.ms/wavlm.},
	number = {6},
	urldate = {2023-12-07},
	journal = {IEEE Journal of Selected Topics in Signal Processing},
	author = {Chen, Sanyuan and Wang, Chengyi and Chen, Zhengyang and Wu, Yu and Liu, Shujie and Chen, Zhuo and Li, Jinyu and Kanda, Naoyuki and Yoshioka, Takuya and Xiao, Xiong and Wu, Jian and Zhou, Long and Ren, Shuo and Qian, Yanmin and Qian, Yao and Wu, Jian and Zeng, Michael and Yu, Xiangzhan and Wei, Furu},
	month = oct,
	year = {2022},
	note = {arXiv:2110.13900 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	pages = {1505--1518},
	annote = {Comment: Submitted to the Journal of Selected Topics in Signal Processing (JSTSP)},
	file = {arXiv.org Snapshot:/Users/khiner/Zotero/storage/WQVU6MUS/2110.html:text/html;Full Text PDF:/Users/khiner/Zotero/storage/WZM9Q9NR/Chen et al. - 2022 - WavLM Large-Scale Self-Supervised Pre-Training fo.pdf:application/pdf},
}
@misc{srivastava_omnivec_2023,
	title = {{OmniVec}: {Learning} robust representations with cross modal sharing},
	shorttitle = {{OmniVec}},
	url = {http://arxiv.org/abs/2311.05709},
	abstract = {Majority of research in learning based methods has been towards designing and training networks for specific tasks. However, many of the learning based tasks, across modalities, share commonalities and could be potentially tackled in a joint framework. We present an approach in such direction, to learn multiple tasks, in multiple modalities, with a unified architecture. The proposed network is composed of task specific encoders, a common trunk in the middle, followed by task specific prediction heads. We first pre-train it by self-supervised masked training, followed by sequential training for the different tasks. We train the network on all major modalities, e.g.{\textbackslash} visual, audio, text and 3D, and report results on \$22\$ diverse and challenging public benchmarks. We demonstrate empirically that, using a joint network to train across modalities leads to meaningful information sharing and this allows us to achieve state-of-the-art results on most of the benchmarks. We also show generalization of the trained network on cross-modal tasks as well as unseen datasets and tasks.},
	urldate = {2023-12-07},
	publisher = {arXiv},
	author = {Srivastava, Siddharth and Sharma, Gaurav},
	month = nov,
	year = {2023},
	note = {arXiv:2311.05709 [cs]
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Accepted to WACV 2024},
}
@inproceedings{groove2019,
    Author = {Jon Gillick and Adam Roberts and Jesse Engel and Douglas Eck and David Bamman},
    Title = {Learning to Groove with Inverse Sequence Transformations},
    Booktitle = {International Conference on Machine Learning (ICML)},
    Year = {2019},
}
@misc{groove2020,
    title={Improving Perceptual Quality of Drum Transcription with the Expanded Groove MIDI Dataset},
    author={Lee Callender and Curtis Hawthorne and Jesse Engel},
    year={2020},
    eprint={2004.00188},
    archivePrefix={arXiv},
    primaryClass={cs.SD}
}