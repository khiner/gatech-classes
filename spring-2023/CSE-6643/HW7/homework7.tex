\documentclass[twoside,10pt]{article}
\input{macro.tex}

\begin{document}

\title{CSE 6643 Homework 7}
\author{Sch{\"a}fer, Spring 2023}
\date{Deadline: April 25 Tuesday, 8:00 am}
\maketitle

\begin{itemize}
  \item There are 2 sections in grade scope: Homework 7 and Homework 7 Programming. Submit your answers as a PDF file to Homework 7 (report the results that you obtain using programming by using plots, tables, and a description of your implementation like you would when writing a paper.) and also submit your code in a zip file to Homework 7 Programming. 
  \item Programming questions are posted in Julia. You are allowed to use basic library functions like sorting, plotting, matrix-vector products etc, but nothing that renders the problem itself trivial. Please use your common sense and ask the instructors if you are unsure. 
  You should never add additional packages to the environment.
  \item Late homework incurs a penalty of 20\% for every 24 hours that it is late. Thus, right after the deadline, it will only be worth 80\% credit, and after four days, it will not be worth any credit. 
  \item We recommend the use of LaTeX for typing up your solutions. No credit will be given to unreadable handwriting.
  \item List explicitly with whom in the class you discussed which problem, if any. Cite all external resources that you were using to complete the homework. For details, consult the collaboration policy in the class syllabus on canvas.
\end{itemize}


\section{Convergence of Lanczos iteration [60 pts]}
Consider the Lanczos process for a syummetric matrix $\mtx{A} \in \R^{m \times m}$, with eigenvalues $\lambda_{1} \geq \lambda_{2} \geq \dots \geq \lambda_{m}$.  
We denote by $\mtx{T}_k$ the $k \times k$ tri-diagonal matrix computed by the Lancczos process starting with the vector $\vct{q}_1$. 
Recall that
\begin{equation}
  \mtx{T}_k = \mtx{Q}^T_k \mtx{A} \mtx{Q}_k.
\end{equation}
The largest eigenvalue $\mu_k$ of $\mtx{T}_k$ satisfies
\begin{equation}
  \mu_{k} = \max \limits_{\vct{y} \neq \vct{0}} \frac{\vct{y}^T \mtx{T}_k \vct{y}}{\vct{y}^T \vct{y}}.
\end{equation}
You can use (or prove, for bonus points, in problem 3) that 
\begin{equation}
  \mu_{k} \leq \lambda_1.
\end{equation}
In this exercise you will also need the Chebychev polynomials $T_k(x)$ satisfying  
\begin{align}
  T_k\left(\cos \theta\right) &= \cos\left(k \theta \right),\\
  T_k\left(\cosh y\right) &= \cosh\left(k y \right).
\end{align}
Using the Chebychev polynomials, we define 
\begin{equation}
  p_k(x) = T_{k - 1} \left( - 1 + 2 \frac{x - \lambda_m}{\lambda_2 - \lambda_m}\right).
\end{equation}

\subsection*{(a) [10 pts]}
Consider a polynomial $p(x)$ of degree $k - 1$.
Show that
\begin{equation}
  \mu_k \geq \frac{\vct{q}_1^T p(\mtx{A}) \mtx{A} p(\mtx{A}) \vct{q}_1}{\vct{q}_1^T p(\mtx{A})^2 \vct{q}_1}
\end{equation}

\subsection*{(b) [10 pts]}
Expand $\vct{q}_1$ in a basis of orthonormal eigenvectors $\left\{\vct{z}_i\right\}_{1 \leq i \leq m}$ of $\mtx{A}$.
\begin{equation}
  \vct{q}_1 = y_1 \vct{z}_1 + \dots + y_m \vct{z}_m. 
\end{equation}
Prove that 
\begin{equation}
  \frac{\vct{q}_1^T p(\mtx{A}) \mtx{A} p(\mtx{A}) \vct{q}_1}{\vct{q}_1^T p(\mtx{A})^2 \vct{q}_1} = \frac{\sum \limits_{i = 1}^{m} y_{i}^2 p(\lambda_i)^2 \lambda_{i}}{\sum \limits_{i = 1}^{m} y_{i}^2 p(\lambda_i)^2}
\end{equation}

\subsection*{(c) [10 pts]}
Show that $|p_k(\lambda_i)| \leq 1$, $\forall 2 \leq i \leq m$. 

\subsection*{(d) [10 pts]}
Show that
\begin{equation}
  p_k(\lambda_1) \in O\left[ \left(2 \left(2 \frac{\lambda_1 - \lambda_2}{\lambda_2 - \lambda_m} + 1\right)\right)^{k - 1} \right],
\end{equation}
in the limit of $(\lambda_{1} - \lambda_{2}) / (\lambda_{2} - \lambda_{m}) \rightarrow \infty$. 

\subsection*{(e) [10 pts]}
Show that with $\cos{\phi_1} = \vct{q}_1^T \vct{z}_1$, 
\begin{equation}
  \lambda_{1} - (\lambda_{1} - \lambda_{m}) \frac{(\tan (\phi_1))^2}{p_{k}(\lambda_1)^2} \leq \mu_{k} \leq \lambda_1.
\end{equation}

\subsection*{(f) [10 pts]}

Assume, for example, that $\lambda_1 - \lambda_2 = (\lambda_2 - \lambda_m) / 2.$  
Find $z$ such that 
\begin{equation}
  p_k (\lambda_1) \approx \frac{z^{k - 1}}{2}.
\end{equation}
Estimate the number of iterations to reach machine accuracy $u$ for $\lambda_1$. 

\section{Arnoldi vs Lanczos [40 pts]}
\subsection*{Arnoldi [15 pts]} 
Go to section (a) of the file \texttt{HW7\_your\_code.jl} and implement a function that implements the Arnoldi method. 
Your method should return an upper Hessenberg matrix. 

\subsection*{(b) Lanczos [15 pts]} 
Go to section (b) of the file \texttt{HW7\_your\_code.jl} and implement a function that implements the Lanczos method. 
Your method should return an upper Hessenberg matrix. 

\subsection*{(c) Arnoldi vs Lanczos [10 pts]}
Compare the behavior of the Arnoldi and Lanczos method on different symmetric matrices. 
How does the approximation improve as you increase $k_{\max}$? 
Can you create reproduce scenarios where Lanczos methods fails, for instance by producing ghost eigenvalues?
Summarize your findings using figures as appropriate, and hand them in \underline{with the written part of your homework}. 

\section{Nested Approximation [20 bonus pts]}
Let $\mtx{A} \in \R^{m \times m}$ be symmetric, and $\mtx{Q} \in \R^{m \times m}$ be orthogonal with columns $\vct{q}_i$. 
Define $\mtx{Q}_k \coloneqq \mtx{Q}_{:, 1 : k}$. 
We denote 
\begin{align}
  \overline{\mu}_k &\coloneqq \lambda_{\max} \left(\mtx{Q}_k^T \mtx{A} \mtx{Q}_k\right),\\
  \underline{\mu}_k &\coloneqq \lambda_{\min}\left(\mtx{Q}^T_k \mtx{A} \mtx{Q}_k\right),
\end{align}
where $\lambda_{\max}$ and $\lambda_{\min}$ are the largest and smallest eigenvalues.

Prove that
\begin{align}
  \overline{\mu}_{1} \leq \overline{\mu}_{2} \leq \dots \leq \overline{\mu}_m &= \lambda_{\max} \left(\mtx{A}\right),\\
  \underline{\mu}_{1} \geq \underline{\mu}_{2} \geq \dots \geq \underline{\mu}_m &= \lambda_{\min} \left(\mtx{A}\right).
\end{align}

\section{Early Termination [20 bonus pts]}
Let $\mtx{A} \in \R^{m \times m}$ be a symmetric matrix. 
We denote by $K$ the dimension of the Krylov subspace 
\begin{equation}
  \mathcal{K}\left(\mtx{A}, \vct{q}_1, m\right) = \left\{\vct{q}_1, \mtx{A}\vct{q}_1, \mtx{A}^2 \vct{q}_1, \ldots, \mtx{A}^{m - 1} \vct{q}_1\right\}.
\end{equation}
The Lanczos algorithm is said to terminate after $k$ steps if $\beta_{k} = 0$ and $\beta_{i} \neq 0, \forall i < k$.

Prove that the Lanczos process, with starting vectors $\vct{q}_1$, terminates in at most $K$ steps in exact arithmetic. 

\section{CIOS [10 bonus pts for the entire class]}
  In order to help the TAs and myself in improving the course, please make sure to fill out your CIOS. 
  I take this feedback into account for future iterations of this class, so please aim to provide me with feedback both on what needs improvement and what is good as is. 
  If $\geq 75$ percent of students in this class fill out their CIOS by the time the homework is graded, everyone in the class earns 10 bonus points.
\end{document}