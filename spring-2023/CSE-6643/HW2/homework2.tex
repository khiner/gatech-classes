\documentclass[twoside,10pt]{article}
\input{macro.tex}

\begin{document}

\title{CSE 6643 Homework 2}
\author{Sch{\"a}fer, Spring 2023}
\date{Deadline: Feb. 3 Friday, 8:00 am}
\maketitle

\begin{itemize}
  \item There are 2 sections in grade scope: Homework 2 and Homework 2 Programming. Submit your answers as a PDF file to Homework 2 (report the results that you obtain using programming by using plots, tables, and a description of your implementation like you would when writing a paper.) and also submit your code in a zip file to Homework 2 Programming. 
  \item Programming questions are posted in Julia. You are allowed to use basic library functions like sorting, plotting, matrix-vector products etc, but nothing that renders the problem itself trivial. Please use your common sense and ask the instructors if you are unsure. 
  You should never add additional packages to the environment.
  \item Late homework incurs a penalty of 20\% for every 24 hours that it is late. Thus, right after the deadline, it will only be worth 80\% credit, and after four days, it will not be worth any credit. 
  \item We recommend the use of LaTeX for typing up your solutions. No credit will be given to unreadable handwriting.
  \item List explicitly with whom in the class you discussed which problem, if any. Cite all external resources that you were using to complete the homework. For details, consult the collaboration policy in the class syllabus on canvas.
\end{itemize}

\section{Less positive [12.5 pts]} 
Consider a symmetric positive definite matrix $\mtx{A}$ separated into subblocks according to
\begin{equation*}
  \mtx{A} = \begin{pmatrix}
    \mtx{A}_{1, 1} & \mtx{A}_{1, 2} \\
    \mtx{A}_{2, 1} & \mtx{A}_{2, 2}
  \end{pmatrix}.
\end{equation*}
Show that the matrix $\mtx{A}_{2,2} - \mtx{A}_{2,1} \mtx{A}_{1,1}^{-1} \mtx{A}_{1,2}$ is symmetric and positive definite. 

\section{Just one [12.5 pts]}
Consider a matrix $\mtx{A} \in \R^{m \times m}$, with (unpivoted) LU-factorization given by $\mtx{A} = \mtx{L} \mtx{U}$. 
Propose an algorithm for computing the $(i, j)$-th entry of $\mtx{A}^{-1}$ using only $\mathcal{O}\left(\left(m + 1 - j\right)^2 + \left(m + 1 - i\right)^2\right)$ floating point operations. 

\section{SVD [25 pts]}
In class, we have reminded ourselves of the SVD of square matrices. 
Use the recommended literature from the class syllabus to catch up on the SVD applied to non-square matrices. 

We denote as $\sigma_{\mathrm{min}}$ and $\sigma_{\mathrm{max}}$ the smallest and largest singular value of a matrix and by $\lambda_{i}$ its $i$-th eigenvalue. 
We consider general matrices $\mtx{A} \in \R^{m \times n}$ and $\mtx{B} \in \R^{n \times l}$.
Show that the following results hold 

\subsection*{(a) [5 pts]}
  \begin{equation*}
    \sigma_{\mathrm{max}}(\mtx{A}) \|\vct{x}\| \geq \|\mtx{A} \vct{x}\|_2, \forall \vct{x} \in \R^n
  \end{equation*}
\subsection*{(b) [5 pts]}
  \begin{equation*}
    m \geq n \Rightarrow \|\mtx{A}\vct{x}\|_2 \geq \sigma_{\mathrm{min}}\left(\mtx{A}\right) \|\vct{x}\|_2, \forall \vct{x} \in \R^n
  \end{equation*}
\subsection*{(c) [5 pts]}
  \begin{equation*}
    m = n \Rightarrow \sigma_{\mathrm{min}}\left(\mtx{A}\right) \leq \left|\lambda_{i}\left(\mtx{A}\right)\right| \leq \sigma_{\mathrm{max}}\left(\mtx{A}\right), \forall i 
  \end{equation*}
\subsection*{(d) [5 pts]}
  \begin{equation*}
    \sigma_{\mathrm{max}}\left(\mtx{A}\right) = 1 / \sigma_{\mathrm{min}}\left(\mtx{A}^{-1}\right)\text{, if $\mtx{A}^{-1}$ exists}
  \end{equation*}
\subsection*{(e) [5 pts]}
  \begin{equation*}
    \text{Find the most general conditions on $m$, $n$, and $l$, under which } \sigma_{\mathrm{min}}\left(\mtx{A}\mtx{B}\right) \geq \sigma_{\mathrm{min}}\left(\mtx{A}\right) \sigma_{\mathrm{min}}\left(\mtx{B}\right).
  \end{equation*}

\section{Inverses [25 pts]}
We consider $\mtx{A} \in \R^{m \times m}$ and assume that $\mtx{A}^{-1}$ exists. 
You may use without proof that if $\mtx{A}$ is symmetric, its largest and smallest eigenvalues are characterized as 
\begin{equation*}
   \lambda_{\mathrm{max}}\left(\mtx(A)\right) = \sup \limits_{\vct{x} \in \R^m \setminus \{\vct{0}\}} \frac{\vct{x}^T \mtx{A}\vct{x}}{\|\vct{x}\|_2^2}
\end{equation*}
and 
\begin{equation*}
   \lambda_{\mathrm{min}}\left(\mtx(A)\right) = \inf \limits_{\vct{x} \in \R^m \setminus \{\vct{0}\}} \frac{\vct{x}^T \mtx{A}\vct{x}}{\|\vct{x}\|_2^2}
\end{equation*}


\subsection*{(a) [5 pts]}
Show that 
\begin{equation}
  \left\|\mtx{A}^{-1}\right\|_2^{-1} = \inf \limits_{\vct{x} \in \R^m \setminus \{\vct{0}\}} \frac{\|\mtx{A}\vct{x}\|_2}{\|\vct{x}\|_2}
\end{equation}

\subsection*{(b) [5 pts]}
Show that 
\begin{equation}
  \left\|\mtx{A}^{-1}\right\|_2^{-1} \geq \inf \limits_{\vct{x} \in \R^m \setminus \{\vct{0}\}} \frac{|\vct{x}^T \mtx{A}\vct{x}|}{\|\vct{x}\|_2^2}
\end{equation}

\subsection*{(c) [5 pts]}

Show that for symmetric matrices $\mtx{M}, \mtx{N} \in \R^{m \times m}$, 
\begin{equation}
  \lambda_{\mathrm{min}}(\mtx{M} + \mtx{N}) \geq \lambda_{\mathrm{min}}\left(\mtx{M}\right) + \lambda_{\mathrm{min}}\left(\mtx{N}\right)
\end{equation}


\subsection*{(d) [5 pts]}
In the same setting as (c), show that 
\begin{equation}
  \lambda_{\mathrm{max}}(\mtx{M} + \mtx{N}) \leq \lambda_{\mathrm{max}}\left(\mtx{M}\right) + \lambda_{\mathrm{max}}\left(\mtx{N}\right)
\end{equation}


\subsection*{(e) [5 pts]}
For $\mtx{A}$ symmetric positive definite, show that 
\begin{equation*}
  \|\mtx{A}^{-1}\|_{2}^{-1} = \inf \limits_{\vct{x} \in \R^m \setminus \{\vct{0}\}} \frac{\vct{x}^T \mtx{A} \vct{x}}{\|\vct{x}\|^2_2}
\end{equation*}

\section{Oblivion [25 pts]}
\subsection*{(a) [5 pts]} 
Have a look at the function \texttt{add\_to\_A\_B\_times\_C!} in \texttt{HW2\_your\_code.jl}. 
This function adds the product of the input variables \texttt{B} and \texttt{C} to the input variable \texttt{A}. 
It is written such as to use the optimal loop order, and employs multiple optimizations by means of the \texttt{@turbo} macro.

Go to section (a) of the file \texttt{HW2\_driver.jl} to see how this function performs when compared to the built-in \texttt{mul!} function. 
If necessary, adapt the size of the matrix to your system. 
Hand in the file \texttt{performance\_per\_size.pdf} produced by the code with your homework. 

\subsection*{(b)locked [5 pts]} 
Now implement a blocked/tiled variant of \texttt{add\_to\_A\_B\_times\_C!} that takes an integer \texttt{bks} as a fourth input.
This method should perform the matrix multiplication by calling the original \texttt{add\_to\_A\_B\_times\_C!} on blocks of size (roughly) \texttt{bks} times \texttt{bks}.
Again, your code should be correct and avoid memory allocations as checked by section (b) of \texttt{HW2\_driver.jl}.

\subsection*{(c) [5 pts]}
Use section \texttt{HW2\_driver.jl} to try the performance of the blocked algorithm for different matrix sizes and block sizes. 
Describe your results, and for the most interesting set of parameters, hand in the resulting plot with your homework.
Describe and explain the results.

\subsection*{(d) oblivious [5 pts]}
We can sometimes obtain better performance by using a hierarchical algorithm. 
Complete the skeleton for \texttt{oblivious\_add\_to\_A\_B\_times\_C!} to obtain an algorithm that equally divides each of its input matrices into four parts and then recurses on the resulting eight subproblems, until one of the problems reaches a size below \texttt{bks}.
Again, make sure that your algorithm does not allocate memory and is correct using part (d) \texttt{HW2\_driver.jl}.

\subsection*{(e) [5 pts]} Use part (e) of \texttt{HW2\_driver.jl} to benchmark your new algorithm for different values of \texttt{bks}. 
Are you able to obtain an improvement over the blocked algorithm? 
Hand in the figure produced by part (e) of \texttt{HW2\_driver.jl} for what you consider the most interesting set of parameters.
Algorithms of this type are called ``cache-oblivious.'' 
Explain why this is the case and what could be the theoretical advantages of this particular cache-oblivious algorithm.

% 
% \subsection*{(b) Matrix-vector-multiplication [5 pts]} 
% Complete the function \texttt{u\_is\_A\_times\_v!(u, A, v)} in \texttt{HW1\_your\_code.jl} that overwrites the input vector \texttt{u} with the product of the input matrix \texttt{A} and the input vector \texttt{v}.
% 
% \subsection*{(c) Matrix-matrix-multiplication [5 pts]} 
% Complete the function \texttt{A\_is\_B\_times\_C!(A, B, C)} in \texttt{HW1\_your\_code.jl} that overwrites the input matrix \texttt{A} with the product of the input matrices \texttt{B} and \texttt{C}.
% 
% \subsection*{(d) Testing [5 pts]}
% From the directory \texttt{HW1\_CODE}, run the command 
% \begin{verbatim}
%   julia --project=. HW1_driver.jl
% \end{verbatim}
% to test your code. Here, the \texttt{-\,-\,project=.} tells Julia to use \texttt{Manifest.toml} and \texttt{Project.toml} determine which version (if any) of packages to use.
% Make sure that your code passes all \texttt{@assert} statements, which test your functions against Julia's built-in functions.
% 
% \subsection*{(e) Optimization [5 pts]} 
% Make sure that your code does not allocate any memory, as evidenced by the \texttt{@btime} calls in \texttt{HW1\_driver.jl} returning \texttt{(0\,allocations:\,0\,bytes)}.
% This is important for performance reasons since allocating memory may be orders of magnitudes slower than floating-point arithmetic. 
% Now try reordering the for-loops in your implementations from parts (a) and (b) and observe the resulting timings provided by \texttt{@btime}. 
% Which order leads to the best and worst performance? 
% What are the corresponding wall-clock times as measured by \texttt{@btime}? 

















%\bibliographystyle{plain}
%\bibliography{temp,externalPapers,groupPapers}

\end{document}
