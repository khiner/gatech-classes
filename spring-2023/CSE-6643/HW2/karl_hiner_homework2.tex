\documentclass[twoside,10pt]{article}
\input{macro.tex}

\begin{document}

\title{CSE 6643 Homework 2}
\author{Karl Hiner}
\date{}
\maketitle

\begin{itemize}
  \item There are 2 sections in grade scope: Homework 2 and Homework 2 Programming. Submit your answers as a PDF file to Homework 2 (report the results that you obtain using programming by using plots, tables, and a description of your implementation like you would when writing a paper.) and also submit your code in a zip file to Homework 2 Programming. 
  \item Programming questions are posted in Julia. You are allowed to use basic library functions like sorting, plotting, matrix-vector products etc, but nothing that renders the problem itself trivial. Please use your common sense and ask the instructors if you are unsure. 
  You should never add additional packages to the environment.
  \item Late homework incurs a penalty of 20\% for every 24 hours that it is late. Thus, right after the deadline, it will only be worth 80\% credit, and after four days, it will not be worth any credit. 
  \item We recommend the use of LaTeX for typing up your solutions. No credit will be given to unreadable handwriting.
  \item List explicitly with whom in the class you discussed which problem, if any. Cite all external resources that you were using to complete the homework. For details, consult the collaboration policy in the class syllabus on canvas.
\end{itemize}

\section{Less positive [12.5 pts]} 
Consider a symmetric positive definite matrix $\mtx{A}$ separated into subblocks according to
\begin{equation*}
  \mtx{A} = \begin{pmatrix}
    \mtx{A}_{1, 1} & \mtx{A}_{1, 2} \\
    \mtx{A}_{2, 1} & \mtx{A}_{2, 2}
  \end{pmatrix}.
\end{equation*}
Show that the matrix $\mtx{A}_{2,2} - \mtx{A}_{2,1} \mtx{A}_{1,1}^{-1} \mtx{A}_{1,2}$ is symmetric and positive definite. 

\quad First, note that $\mtx{A}_{2,2} - \mtx{A}_{2,1} \mtx{A}_{1,1}^{-1} \mtx{A}_{1,2}$ is the \textit{Schur complement} of $\mtx{A}$, which we will refer to as $\mtx{S}$ for brevity.
For $\mtx{S}$ to be symmetric, we must show that $\mtx{S}^T = \mtx{S}:$ 

\begin{align*}
\mtx{S}^T &= \left(\mtx{A}_{2,2} - \mtx{A}_{2,1} \mtx{A}_{1,1}^{-1} \mtx{A}_{1,2}\right)^T\\
&= \mtx{A}_{2,2}^T - \mtx{A}_{1,2}^T \left(\mtx{A}_{1,1}^{-1}\right)^T \mtx{A}_{2,1}^T \\
&= \mtx{A}_{2,2} - \mtx{A}_{2,1} \mtx{A}_{1,1}^{-1} \mtx{A}_{1,2} = \mtx{S}
\end{align*}

\quad Since $\mtx{A}$ is symmetric, $\mtx{A}_{1,2} = \mtx{A}_{2,1}^T$, $\mtx{A}_{2,1} = \mtx{A}_{1,2}^T$, and $\mtx{A}_{2,2} = \mtx{A}_{2,2}^T$.
Since $\mtx{A}$ is positive definite, it is invertible, and since it is symmetric, then its inverse is also symmetric, and $\left(\mtx{A}_{1,1}^{-1}\right)^T = \mtx{A}_{1,1}^{-1}$.
Thus, the above equality is satisfied and $\mtx{S}$ is symmetric.

To show that $\mtx{S}$ is positive definite, we need to show that for any non-zero vector $\vct{x}$, $\vct{x}^T \mtx{S} \vct{x} > 0$.

First, keeping in mind that $\mtx{A}_{2,1} = \left(\mtx{A}_{1,2}\right)^T$, let's define $\mtx{A}$ more compactly as:

\begin{equation*}
  \mtx{A} = \begin{pmatrix}
    \mtx{B} & \mtx{C} \\
    \mtx{C}^T & \mtx{D}
  \end{pmatrix}.
\end{equation*}

Then the Schur complement is $\mtx{S} = \mtx{D} - \mtx{C}^T \mtx{B}^{-1} \mtx{C}$, and we can express $\mtx{A}$ as

\begin{equation*}
  \mtx{A} = \begin{pmatrix}
    \mtx{I} & \mtx{0} \\
    \mtx{C}^T\mtx{B}^{-1} & \mtx{I}
  \end{pmatrix}
  \begin{pmatrix}
    \mtx{B} & \mtx{0} \\
    \mtx{0} & \mtx{S}
  \end{pmatrix}
  \begin{pmatrix}
    \mtx{I} & \mtx{B}^{-1}\mtx{C} \\
    \mtx{0} & \mtx{I}
  \end{pmatrix}
  =
  \mtx{N}
  \begin{pmatrix}
    \mtx{B} & \mtx{0} \\
    \mtx{0} & \mtx{S}
  \end{pmatrix}
  \mtx{N}^T.
\end{equation*}

$\mtx{N}$ is invertible, and its inverse can be found like an elementary real matrix, by negating its single off-diagonal element:

\begin{align*}
  \mtx{N} &=
  \begin{pmatrix}
    \mtx{I} & \mtx{0} \\
    \mtx{C}^T\mtx{B}^{-1} & \mtx{I}
  \end{pmatrix}\\
  \mtx{N}^{-1} &=
  \begin{pmatrix}
    \mtx{I} & \mtx{0} \\
    -\mtx{C}^T\mtx{B}^{-1} & \mtx{I}
  \end{pmatrix}
\end{align*}

Let $\mtx{M} \equiv \begin{pmatrix}
    \mtx{B} & \mtx{0} \\
    \mtx{0} & \mtx{S}
  \end{pmatrix}.$

Then,
\begin{align*}
  \mtx{M} &=
  \mtx{N}^{-1}\mtx{A}\left(\mtx{N}^{-1}\right)^T\\
  \vct{u}^T\mtx{M}\vct{u}
  &=
  \vct{u}^T\mtx{N}^{-1}\mtx{A}\left(\mtx{N}^{-1}\right)^T\vct{u} &\quad \text{(for any vector $\vct{u} \not\equiv \vct{0}$)}\\
  &=
  \vct{v}^T\mtx{A}\vct{v} &\quad (\vct{v} \equiv \left(\mtx{N}^{-1}\right)^T\vct{u})\\
  &> 0 &\quad \text{(since $\mtx{A}$ is positive definite)}
\end{align*}

Thus, $\mtx{M}$ is positive-definite.

Since $\mtx{A}$ is positive definite, then $\mtx{B}$ must be positive definite, since it is a principle submatrix of $\mtx{A}$ (and of $\mtx{M}$, which we just showed to also be positive definite!).

Thus, $\mtx{S} = \mtx{D} - \mtx{C}^T \mtx{B}^{-1} \mtx{C} = \mtx{A}_{2,2} - \mtx{A}_{2,1} \mtx{A}_{1,1}^{-1} \mtx{A}_{1,2}$ is also positive definite, since it is the other principle submatrix of the positive definite $\mtx{M}$.

\section{Just one [12.5 pts]}
Consider a matrix $\mtx{A} \in \R^{m \times m}$, with (unpivoted) LU-factorization given by $\mtx{A} = \mtx{L} \mtx{U}$. 
Propose an algorithm for computing the $(i, j)$-th entry of $\mtx{A}^{-1}$ using only $\mathcal{O}\left(\left(m + 1 - j\right)^2 + \left(m + 1 - i\right)^2\right)$ floating point operations. 

\quad The algorithm for computing the $(i, j)$-th entry of $\mtx{A}^{-1}$ is as follows:

\begin{enumerate}
\item Compute the $j$-th column of $\mtx{A}^{-1}$ by solving $\mtx{L} \mtx{y} = \mtx{e}_j$ for $\mtx{y}$, where $\mtx{e}_j$ is the $j$-th standard basis vector.
This step takes $\mathcal{O}(\left(m + 1 - j\right)^2)$ operations.
\item Compute the $(i, j)$-th entry of $\mtx{A}^{-1}$ by solving $\mtx{U} \mtx{x} = \mtx{y}$ for $\mtx{x}$, where the $i$-th entry of $\mtx{x}$ is the desired entry.
This step takes $\mathcal{O}(\left(m + 1 - i\right)^2)$ operations.
\end{enumerate}

\quad The total number of operations is $\mathcal{O}(\left(m + 1 - j\right)^2 + \left(m + 1 - i\right)^2)$.

\section{SVD [25 pts]}
In class, we have reminded ourselves of the SVD of square matrices. 
Use the recommended literature from the class syllabus to catch up on the SVD applied to non-square matrices. 

We denote as $\sigma_{\mathrm{min}}$ and $\sigma_{\mathrm{max}}$ the smallest and largest singular value of a matrix and by $\lambda_{i}$ its $i$-th eigenvalue. 
We consider general matrices $\mtx{A} \in \R^{m \times n}$ and $\mtx{B} \in \R^{n \times l}$.
Show that the following results hold 

\subsection*{(a) [5 pts]}
  \begin{equation*}
    \sigma_{\mathrm{max}}(\mtx{A}) \|\vct{x}\|_2 \geq \|\mtx{A} \vct{x}\|_2, \forall \vct{x} \in \R^n
  \end{equation*}

\begin{align*}
 \|\mtx{A} \vct{x}\|_2 &\leq \|\mtx{A}\|_2\|\vct{x}\|_2 &\quad\text{(consistency of $\|\cdot\|_2$ operator norm with induced vector norm)}\\
 &= \sigma_{\mathrm{max}}(\mtx{A})\|\vct{x}\|_2 &\quad\text{(Thm. 5.3 in Trefethen \& Bau: $\|\mtx{A}\|_2 = \sigma_{\mathrm{max}}(\mtx{A})$)}
\end{align*}

TODO must prove Thm. 5.3

\subsection*{(b) [5 pts]}
  \begin{equation*}
    m \geq n \Rightarrow \|\mtx{A}\vct{x}\|_2 \geq \sigma_{\mathrm{min}}\left(\mtx{A}\right) \|\vct{x}\|_2, \forall \vct{x} \in \R^n
  \end{equation*}

\begin{align*}
  \|\mtx{A}\vct{x}\|_2 &\geq \sigma_{\mathrm{min}}\left(\mtx{A}\right) \|\vct{x}\|_2\\
  \|\mtx{A}\vct{x}\|_2 &\geq \sqrt{\lambda_{\mathrm{min}}(\mtx{A}^T\mtx{A})} \|\vct{x}\|_2 &\text{(def. of singular values)}\\
  \|\mtx{A}\vct{x}\|_2^2 &\geq \lambda_{\mathrm{min}}(\mtx{A}^T\mtx{A}) \|\vct{x}\|_2^2 &\text{(square both sides)}\\
  \lambda_{\mathrm{max}}((\mtx{A}\vct{x})^T(\mtx{A}\vct{x})) &\geq \lambda_{\mathrm{min}}(\mtx{A}^T\mtx{A}) \|\vct{x}\|_2^2 &\text{($\|\mtx{A}\|_2 = \sqrt{\lambda_{\mathrm{max}}(\mtx{A^T}\mtx{A})}$)}\\
  \lambda_{\mathrm{max}}(\vct{x}^T\mtx{A}^T\mtx{A}\vct{x}) &\geq \lambda_{\mathrm{min}}(\mtx{A}^T\mtx{A}) (\vct{x}^T\vct{x})  &\text{(simplify)}
\end{align*}

   $\lambda_{\mathrm{min}}\left(\mtx{A}\right) = \inf \limits_{\vct{x} \in \R^m \setminus \{\vct{0}\}} \dfrac{\vct{x}^T \mtx{A}\vct{x}}{\|\vct{x}\|_2^2}$

  Note that the ranks of $A^TA$ and $AA^T$ are different (and thus one of them contains at least one eigenvalue $0$ if $m > n$ or $m < n$.

\subsection*{(c) [5 pts]}
  \begin{equation*}
    m = n \Rightarrow \sigma_{\mathrm{min}}(\mtx{A}) \leq \left|\lambda_{i}\left(\mtx{A}\right)\right| \leq \sigma_{\mathrm{max}}\left(\mtx{A}\right), \forall i 
  \end{equation*}

  First, we prove $\left|\lambda_{i}(\mtx{A})\right| \leq \sigma_{\mathrm{max}}\left(\mtx{A}\right)$.

  Let $\lambda$ be an eigenvalue of $\mtx{A}$, and let $\vct{x}$ be the corresponding unit eigenvector such that $\mtx{A}\vct{x} = \lambda\vct{x}$. 
  Taking the 2-norm of both sides:
  \begin{align*}
    \|\mtx{A}\vct{x}\|_2 &= \|\lambda \vct{x}\|_2\\
    &= |\lambda| \|\vct{x}\|_2\\
    &= |\lambda|\quad\quad\quad\text{(since $\|\vct{x}\|_2 = 1$)}
  \end{align*}
  
  By definition,
 
\begin{equation*}
\sigma_{\mathrm{max}}(\mtx{A}) = \sup_{\|x\|_2 = 1}{\|\mtx{A}\vct{x}\|_2},
\end{equation*}

and so:
  
\begin{equation*}
  \sigma_{\mathrm{max}}(\mtx{A}) \geq \|\mtx{A}\vct{x}\|_2 = |\lambda_i(\mtx{A})|, \forall i
\end{equation*}

For the other side, $\sigma_{\mathrm{min}}(\mtx{A}) \leq \left|\lambda_{i}\left(\mtx{A}\right)\right|$,

... TODO use (d) below. If $\mtx{A}$ is singular, its smallest singular value is zero and the inequality is trivially true.

\subsection*{(d) [5 pts]
  \begin{equation*}
    \sigma_{\mathrm{max}}\left(\mtx{A}\right) = 1 / \sigma_{\mathrm{min}}\left(\mtx{A}^{-1}\right)\text{, if $\mtx{A}^{-1}$ exists}
  \end{equation*}
\subsection*{(e) [5 pts]}
  \begin{equation*}
    \text{Find the most general conditions on $m$, $n$, and $l$, under which } \sigma_{\mathrm{min}}\left(\mtx{A}\mtx{B}\right) \geq \sigma_{\mathrm{min}}\left(\mtx{A}\right) \sigma_{\mathrm{min}}\left(\mtx{B}\right).
  \end{equation*}

\section{Inverses [25 pts]}
We consider $\mtx{A} \in \R^{m \times m}$ and assume that $\mtx{A}^{-1}$ exists. 
You may use without proof that if $\mtx{A}$ is symmetric, its largest and smallest eigenvalues are characterized as 
\begin{equation*}
   \lambda_{\mathrm{max}}\left(\mtx(A)\right) = \sup \limits_{\vct{x} \in \R^m \setminus \{\vct{0}\}} \frac{\vct{x}^T \mtx{A}\vct{x}}{\|\vct{x}\|_2^2}
\end{equation*}
and 
\begin{equation*}
   \lambda_{\mathrm{min}}\left(\mtx(A)\right) = \inf \limits_{\vct{x} \in \R^m \setminus \{\vct{0}\}} \frac{\vct{x}^T \mtx{A}\vct{x}}{\|\vct{x}\|_2^2}
\end{equation*}


\subsection*{(a) [5 pts]}
Show that 
\begin{equation}
  \left\|\mtx{A}^{-1}\right\|_2^{-1} = \inf \limits_{\vct{x} \in \R^m \setminus \{\vct{0}\}} \frac{\|\mtx{A}\vct{x}\|_2}{\|\vct{x}\|_2}
\end{equation}

\begin{align*}
  \mtx{A} &= \mtx{U} \mtx{\Sigma} \mtx{V}^T&\quad\text{(SVD decomposition of $\mtx{A}$)}\\
  \mtx{A}^{-1} &= \mtx{V} \mtx{\Sigma}^{-1} \mtx{U}^T&\quad\text{(invert both sides)}\\
  \left\|\mtx{A}^{-1}\right\|_2 &= \left\|\mtx{V} \mtx{\Sigma}^{-1} \mtx{U}^T\right\|_2&\quad\text{(take the 2-norm of both sides)}\\
  &= \sigma_{\mathrm{max}}\left(\mtx{V} \mtx{\Sigma}^{-1} \mtx{U}^T\right)&\quad\text{(since $\left\|\mtx{B}\right\|_2 = \sigma_{\mathrm{max}}\left(\mtx{B}\right)$ for any matrix $\mtx{B}$)}\\
  &= \sigma_{\mathrm{max}}\left(\mtx{\Sigma}^{-1}\right)&\quad\text{(since $\mtx{V}$ and $\mtx{U}$ are orthogonal)}\\
  &= \frac{1}{\sigma_{\mathrm{min}}\left(\mtx{\Sigma}\right)}&\quad\text{(\textit{3d} above)}\\
  &= \frac{1}{\sigma_{\mathrm{min}}\left(\mtx{A}\right)}&\quad\text{(def. of $\mtx{\Sigma}$)}\\
  \left\|\mtx{A}^{-1}\right\|_2^{-1} &= \sigma_{\mathrm{min}}\left(\mtx{A}\right)&\quad\text{(invert both sides)}\\
  &=  \inf \limits_{\vct{x} \in \R^m \setminus \{\vct{0}\}} \frac{\|\mtx{A}\vct{x}\|_2}{\|\vct{x}\|_2}&\quad\text{(def. of $\sigma_\mathrm{min}$)}
\end{align*}

\subsection*{(b) [5 pts]}
Show that 
\begin{equation}
  \left\|\mtx{A}^{-1}\right\|_2^{-1} \geq \inf \limits_{\vct{x} \in \R^m \setminus \{\vct{0}\}} \frac{|\vct{x}^T \mtx{A}\vct{x}|}{\|\vct{x}\|_2^2}
\end{equation}

This follows directly from \textit{3c} and \textit{4a} above, along with the given definition of $\lambda_{\mathrm{min}}(\mtx{A})$:

\begin{align*}
  \left\|\mtx{A}^{-1}\right\|_2^{-1} &= \sigma_{\mathrm{min}}\left(\mtx{A}\right)&\quad\text{(From \textit{4a})}\\
  &\geq&\quad\text{(from \textit{3c}, since $\mtx{A}$ is square $m = n \Rightarrow \sigma_{\mathrm{min}}(\mtx{A}) \leq \left|\lambda_{i}\left(\mtx{A}\right)\right|$)}
\end{align*}


\subsection*{(c) [5 pts]}

Show that for symmetric matrices $\mtx{M}, \mtx{N} \in \R^{m \times m}$, 
\begin{equation}
  \lambda_{\mathrm{min}}(\mtx{M} + \mtx{N}) \geq \lambda_{\mathrm{min}}\left(\mtx{M}\right) + \lambda_{\mathrm{min}}\left(\mtx{N}\right)
\end{equation}


\subsection*{(d) [5 pts]}
In the same setting as (c), show that 
\begin{equation}
  \lambda_{\mathrm{max}}(\mtx{M} + \mtx{N}) \leq \lambda_{\mathrm{max}}\left(\mtx{M}\right) + \lambda_{\mathrm{max}}\left(\mtx{N}\right)
\end{equation}


\subsection*{(e) [5 pts]}
For $\mtx{A}$ symmetric positive definite, show that 
\begin{equation*}
  \|\mtx{A}^{-1}\|_{2}^{-1} = \inf \limits_{\vct{x} \in \R^m \setminus \{\vct{0}\}} \frac{\vct{x}^T \mtx{A} \vct{x}}{\|\vct{x}\|^2_2}
\end{equation*}

In \textit{4a} above, we showed that 

\begin{equation*}
  \left\|\mtx{A}^{-1}\right\|_2^{-1} = \sigma_{\mathrm{min}}\left(\mtx{A}\right).
\end{equation*}

For symmetric matrices, the singular values are the absolute values of the eigenvalues.

\section{Oblivion [25 pts]}
\subsection*{(a) [5 pts]} 
Have a look at the function \texttt{add\_to\_A\_B\_times\_C!} in \texttt{HW2\_your\_code.jl}. 
This function adds the product of the input variables \texttt{B} and \texttt{C} to the input variable \texttt{A}. 
It is written such as to use the optimal loop order, and employs multiple optimizations by means of the \texttt{@turbo} macro.

Go to section (a) of the file \texttt{HW2\_driver.jl} to see how this function performs when compared to the built-in \texttt{mul!} function. 
If necessary, adapt the size of the matrix to your system. 
Hand in the file \texttt{performance\_per\_size.pdf} produced by the code with your homework. 

\subsection*{(b)locked [5 pts]} 
Now implement a blocked/tiled variant of \texttt{add\_to\_A\_B\_times\_C!} that takes an integer \texttt{bks} as a fourth input.
This method should perform the matrix multiplication by calling the original \texttt{add\_to\_A\_B\_times\_C!} on blocks of size (roughly) \texttt{bks} times \texttt{bks}.
Again, your code should be correct and avoid memory allocations as checked by section (b) of \texttt{HW2\_driver.jl}.

\subsection*{(c) [5 pts]}
Use section \texttt{HW2\_driver.jl} to try the performance of the blocked algorithm for different matrix sizes and block sizes. 
Describe your results, and for the most interesting set of parameters, hand in the resulting plot with your homework.
Describe and explain the results.

\subsection*{(d) oblivious [5 pts]}
We can sometimes obtain better performance by using a hierarchical algorithm. 
Complete the skeleton for \texttt{oblivious\_add\_to\_A\_B\_times\_C!} to obtain an algorithm that equally divides each of its input matrices into four parts and then recurses on the resulting eight subproblems, until one of the problems reaches a size below \texttt{bks}.
Again, make sure that your algorithm does not allocate memory and is correct using part (d) \texttt{HW2\_driver.jl}.

\subsection*{(e) [5 pts]} Use part (e) of \texttt{HW2\_driver.jl} to benchmark your new algorithm for different values of \texttt{bks}. 
Are you able to obtain an improvement over the blocked algorithm? 
Hand in the figure produced by part (e) of \texttt{HW2\_driver.jl} for what you consider the most interesting set of parameters.
Algorithms of this type are called ``cache-oblivious.'' 
Explain why this is the case and what could be the theoretical advantages of this particular cache-oblivious algorithm.

% 
% \subsection*{(b) Matrix-vector-multiplication [5 pts]} 
% Complete the function \texttt{u\_is\_A\_times\_v!(u, A, v)} in \texttt{HW1\_your\_code.jl} that overwrites the input vector \texttt{u} with the product of the input matrix \texttt{A} and the input vector \texttt{v}.
% 
% \subsection*{(c) Matrix-matrix-multiplication [5 pts]} 
% Complete the function \texttt{A\_is\_B\_times\_C!(A, B, C)} in \texttt{HW1\_your\_code.jl} that overwrites the input matrix \texttt{A} with the product of the input matrices \texttt{B} and \texttt{C}.
% 
% \subsection*{(d) Testing [5 pts]}
% From the directory \texttt{HW1\_CODE}, run the command 
% \begin{verbatim}
%   julia --project=. HW1_driver.jl
% \end{verbatim}
% to test your code. Here, the \texttt{-\,-\,project=.} tells Julia to use \texttt{Manifest.toml} and \texttt{Project.toml} determine which version (if any) of packages to use.
% Make sure that your code passes all \texttt{@assert} statements, which test your functions against Julia's built-in functions.
% 
% \subsection*{(e) Optimization [5 pts]} 
% Make sure that your code does not allocate any memory, as evidenced by the \texttt{@btime} calls in \texttt{HW1\_driver.jl} returning \texttt{(0\,allocations:\,0\,bytes)}.
% This is important for performance reasons since allocating memory may be orders of magnitudes slower than floating-point arithmetic. 
% Now try reordering the for-loops in your implementations from parts (a) and (b) and observe the resulting timings provided by \texttt{@btime}. 
% Which order leads to the best and worst performance? 
% What are the corresponding wall-clock times as measured by \texttt{@btime}? 

















%\bibliographystyle{plain}
%\bibliography{temp,externalPapers,groupPapers}

\end{document}
