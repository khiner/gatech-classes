\documentclass[twoside,10pt]{article}
\input{macro.tex}

\newcommand{\q}{\quad?\quad}

\begin{document}

\title{CSE 6643 Homework 2}
\author{Karl Hiner}
\date{}
\maketitle

\begin{itemize}
  \item There are 2 sections in grade scope: Homework 2 and Homework 2 Programming. Submit your answers as a PDF file to Homework 2 (report the results that you obtain using programming by using plots, tables, and a description of your implementation like you would when writing a paper.) and also submit your code in a zip file to Homework 2 Programming. 
  \item Programming questions are posted in Julia. You are allowed to use basic library functions like sorting, plotting, matrix-vector products etc, but nothing that renders the problem itself trivial. Please use your common sense and ask the instructors if you are unsure. 
  You should never add additional packages to the environment.
  \item Late homework incurs a penalty of 20\% for every 24 hours that it is late. Thus, right after the deadline, it will only be worth 80\% credit, and after four days, it will not be worth any credit. 
  \item We recommend the use of LaTeX for typing up your solutions. No credit will be given to unreadable handwriting.
  \item List explicitly with whom in the class you discussed which problem, if any. Cite all external resources that you were using to complete the homework. For details, consult the collaboration policy in the class syllabus on canvas.
\end{itemize}

\section{Less positive [12.5 pts]} 

Consider a symmetric positive definite matrix $\mtx{A}$ separated into subblocks according to
\begin{equation*}
  \mtx{A} = \begin{pmatrix}
    \mtx{A}_{1, 1} & \mtx{A}_{1, 2} \\
    \mtx{A}_{2, 1} & \mtx{A}_{2, 2}
  \end{pmatrix}.
\end{equation*}
Show that the matrix $\mtx{A}_{2,2} - \mtx{A}_{2,1} \mtx{A}_{1,1}^{-1} \mtx{A}_{1,2}$ is symmetric and positive definite. 

\quad First, note that $\mtx{A}_{2,2} - \mtx{A}_{2,1} \mtx{A}_{1,1}^{-1} \mtx{A}_{1,2}$ is the \textit{Schur complement} of $\mtx{A}$, which we will refer to as $\mtx{S}$ for brevity.
For $\mtx{S}$ to be symmetric, we must show that $\mtx{S}^T = \mtx{S}:$ 

\begin{align*}
\mtx{S}^T &= \left(\mtx{A}_{2,2} - \mtx{A}_{2,1} \mtx{A}_{1,1}^{-1} \mtx{A}_{1,2}\right)^T\\
&= \mtx{A}_{2,2}^T - \mtx{A}_{1,2}^T \left(\mtx{A}_{1,1}^{-1}\right)^T \mtx{A}_{2,1}^T \\
&= \mtx{A}_{2,2} - \mtx{A}_{2,1} \mtx{A}_{1,1}^{-1} \mtx{A}_{1,2} = \mtx{S}
\end{align*}

\quad Since $\mtx{A}$ is symmetric, $\mtx{A}_{1,2} = \mtx{A}_{2,1}^T$, $\mtx{A}_{2,1} = \mtx{A}_{1,2}^T$, and $\mtx{A}_{2,2} = \mtx{A}_{2,2}^T$.
Since $\mtx{A}$ is positive definite, it is invertible, and since it is symmetric, then its inverse is also symmetric, and $\left(\mtx{A}_{1,1}^{-1}\right)^T = \mtx{A}_{1,1}^{-1}$.
Thus, the above equality is satisfied and $\mtx{S}$ is symmetric.

To show that $\mtx{S}$ is positive definite, we need to show that for any non-zero vector $\vct{x}$, $\vct{x}^T \mtx{S} \vct{x} > 0$.

First, keeping in mind that $\mtx{A}_{2,1} = \left(\mtx{A}_{1,2}\right)^T$, let's define $\mtx{A}$ more compactly as:

\begin{equation*}
  \mtx{A} = \begin{pmatrix}
    \mtx{B} & \mtx{C} \\
    \mtx{C}^T & \mtx{D}
  \end{pmatrix}.
\end{equation*}

Then the Schur complement is $\mtx{S} = \mtx{D} - \mtx{C}^T \mtx{B}^{-1} \mtx{C}$, and we can express $\mtx{A}$ as

\begin{equation*}
  \mtx{A} = \begin{pmatrix}
    \mtx{I} & \mtx{0} \\
    \mtx{C}^T\mtx{B}^{-1} & \mtx{I}
  \end{pmatrix}
  \begin{pmatrix}
    \mtx{B} & \mtx{0} \\
    \mtx{0} & \mtx{S}
  \end{pmatrix}
  \begin{pmatrix}
    \mtx{I} & \mtx{B}^{-1}\mtx{C} \\
    \mtx{0} & \mtx{I}
  \end{pmatrix}
  =
  \mtx{N}
  \begin{pmatrix}
    \mtx{B} & \mtx{0} \\
    \mtx{0} & \mtx{S}
  \end{pmatrix}
  \mtx{N}^T.
\end{equation*}

$\mtx{N}$ is invertible, and its inverse can be found like an elementary real matrix, by negating its single off-diagonal element:

\begin{align*}
  \mtx{N} &=
  \begin{pmatrix}
    \mtx{I} & \mtx{0} \\
    \mtx{C}^T\mtx{B}^{-1} & \mtx{I}
  \end{pmatrix}\\
  \mtx{N}^{-1} &=
  \begin{pmatrix}
    \mtx{I} & \mtx{0} \\
    -\mtx{C}^T\mtx{B}^{-1} & \mtx{I}
  \end{pmatrix}
\end{align*}

Let $\mtx{M} \equiv \begin{pmatrix}
    \mtx{B} & \mtx{0} \\
    \mtx{0} & \mtx{S}
  \end{pmatrix}.$

Then,
\begin{align*}
  \mtx{M} &=
  \mtx{N}^{-1}\mtx{A}\left(\mtx{N}^{-1}\right)^T\\
  \vct{u}^T\mtx{M}\vct{u}
  &=
  \vct{u}^T\mtx{N}^{-1}\mtx{A}\left(\mtx{N}^{-1}\right)^T\vct{u} &\quad \text{(for any vector $\vct{u} \not\equiv \vct{0}$)}\\
  &=
  \vct{v}^T\mtx{A}\vct{v} &\quad (\vct{v} \equiv \left(\mtx{N}^{-1}\right)^T\vct{u})\\
  &> 0 &\quad \text{(since $\mtx{A}$ is positive definite)}
\end{align*}

Thus, $\mtx{M}$ is positive-definite.

Since $\mtx{A}$ is positive definite, then $\mtx{B}$ must be positive definite, since it is a principle submatrix of $\mtx{A}$ (and of $\mtx{M}$, which we just showed to also be positive definite!).

Thus, $\mtx{S} = \mtx{D} - \mtx{C}^T \mtx{B}^{-1} \mtx{C} = \mtx{A}_{2,2} - \mtx{A}_{2,1} \mtx{A}_{1,1}^{-1} \mtx{A}_{1,2}$ is also positive definite, since it is the other principle submatrix of the positive definite $\mtx{M}$.

\section{Just one [12.5 pts]}
Consider a matrix $\mtx{A} \in \R^{m \times m}$, with (unpivoted) LU-factorization given by $\mtx{A} = \mtx{L} \mtx{U}$. 
Propose an algorithm for computing the $(i, j)$-th entry of $\mtx{A}^{-1}$ using only $\mathcal{O}\left(\left(m + 1 - j\right)^2 + \left(m + 1 - i\right)^2\right)$ floating point operations. 

\quad An algorithm for computing the $(i, j)$-th entry of $\mtx{A}^{-1}$ is as follows:

\begin{enumerate}
\item Compute the $j$-th column of $\mtx{A}^{-1}$ by solving $\mtx{L} \mtx{y} = \mtx{e}_j$ for $\mtx{y}$, where $\mtx{e}_j$ is the $j$-th standard basis vector.
This step takes $\mathcal{O}(\left(m + 1 - j\right)^2)$ operations.
\item Compute the $(i, j)$-th entry of $\mtx{A}^{-1}$ by solving $\mtx{U} \mtx{x} = \mtx{y}$ for $\mtx{x}$, where the $i$-th entry of $\mtx{x}$ is the desired entry.
This step takes $\mathcal{O}(\left(m + 1 - i\right)^2)$ operations.
\end{enumerate}

\quad The total number of operations is $\mathcal{O}(\left(m + 1 - j\right)^2 + \left(m + 1 - i\right)^2)$.

\section{SVD [25 pts]}
In class, we have reminded ourselves of the SVD of square matrices. 
Use the recommended literature from the class syllabus to catch up on the SVD applied to non-square matrices. 

We denote as $\sigma_{\mathrm{min}}$ and $\sigma_{\mathrm{max}}$ the smallest and largest singular value of a matrix and by $\lambda_{i}$ its $i$-th eigenvalue. 
We consider general matrices $\mtx{A} \in \R^{m \times n}$ and $\mtx{B} \in \R^{n \times l}$.
Show that the following results hold 

\subsection*{(a) [5 pts]}
  \begin{equation*}
    \sigma_{\mathrm{max}}(\mtx{A}) \|\vct{x}\|_2 \geq \|\mtx{A} \vct{x}\|_2, \forall \vct{x} \in \R^n
  \end{equation*}

\begin{align*}
 \|\mtx{A} \vct{x}\|_2 &\leq \|\mtx{A}\|_2\|\vct{x}\|_2 &\quad\text{(consistency of $\|\cdot\|_2$ operator norm with induced vector norm)}\\
 &= \sigma_{\mathrm{max}}(\mtx{A})\|\vct{x}\|_2 &\quad\text{(Thm. 5.3 in Trefethen \& Bau: $\|\mtx{A}\|_2 = \sigma_{\mathrm{max}}(\mtx{A})$)}
\end{align*}

TODO must prove Thm. 5.3

\subsection*{(b) [5 pts]}
  \begin{equation*}
    m \geq n \Rightarrow \|\mtx{A}\vct{x}\|_2 \geq \sigma_{\mathrm{min}}\left(\mtx{A}\right) \|\vct{x}\|_2, \forall \vct{x} \in \R^n
  \end{equation*}

\begin{align*}
  \dfrac{\vct{x}^{*}\mtx{A}^{*}\mtx{A}\vct{x}}{\|\vct{x}\|_2^2} &\geq\inf \limits_{\vct{y} \in \R^m \setminus \{\vct{0}\}} \dfrac{\vct{y}^{*} \mtx{A}^{*}\mtx{A}\vct{y}}{\|\vct{y}\|_2^2} &\text{($\forall\vct{x} \in \R^n$)}\\
  \vct{x}^{*}\mtx{A}^{*}\mtx{A}\vct{x} &\geq \left(\inf \limits_{\vct{y} \in \R^m \setminus \{\vct{0}\}} \dfrac{\vct{y}^{*} \mtx{A}^{*}\mtx{A}\vct{y}}{\|\vct{y}\|_2^2}\right) \|\vct{x}\|_2^2  &\text{(multiply both sides by $\|\vct{x}\|_2^2$)}\\
  \vct{x}^{*}\mtx{A}^{*}\mtx{A}\vct{x} &\geq \lambda_{\mathrm{min}}(\mtx{A}^{*}\mtx{A}) \|\vct{x}\|_2^2 &\text{$\left(\lambda_{\mathrm{min}}\left(\mtx{A}\right) = \inf \limits_{\vct{y} \in \R^m \setminus \{\vct{0}\}} \dfrac{\vct{y}^{*} \mtx{A}\vct{y}}{\|\vct{y}\|_2^2}\right)$}\\
  \|\mtx{A}\vct{x}\|_2^2 &\geq \lambda_{\mathrm{min}}(\mtx{A}^{*}\mtx{A}) \|\vct{x}\|_2^2 &\text{($\|\vct{v}\|_2 = \sqrt{\vct{v}^{*}\vct{v}}$)}\\
  \|\mtx{A}\vct{x}\|_2 &\geq \sqrt{\lambda_{\mathrm{min}}(\mtx{A}^{*}\mtx{A})} \|\vct{x}\|_2 &\text{(take square root of both sides)}\\
  &=\sigma_{\mathrm{min}}\left(\mtx{A}\right) \|\vct{x}\|_2 &\text{(def. of singular values)}\\
\end{align*}

  Note that the ranks of $A^TA$ and $AA^T$ are different (and thus one of them contains at least one eigenvalue $0$ if $m > n$ or $m < n$).

\subsection*{(c) [5 pts]}
  \begin{equation*}
    m = n \Rightarrow \sigma_{\mathrm{min}}(\mtx{A}) \leq \left|\lambda_{i}\left(\mtx{A}\right)\right| \leq \sigma_{\mathrm{max}}\left(\mtx{A}\right), \forall i 
  \end{equation*}

  Let $\lambda_i$ be an eigenvalue of $\mtx{A}$, and let $\vct{v_i}$ be a corresponding unit eigenvector such that $\mtx{A}\vct{v_i} = \lambda_i\vct{v_i}$. 
  Taking the 2-norm of both sides:
  \begin{align*}
    \|\mtx{A}\vct{v_i}\|_2 &= \|\lambda_i \vct{v_i}\|_2\\
    &= |\lambda_i| \|\vct{v_i}\|_2\\
    &= |\lambda_i|\quad\quad\quad\text{(since $\|\vct{v_i}\|_2 = 1$)}
  \end{align*}

  Also, note that since $\mtx{A}$ is square when $m = n$, $\mtx{A}^{*}\mtx{A}$ is positive semidefinite, and can thus be decomposed into $\mtx{U}\mtx{\Lambda}\mtx{U}^{*}$, where $\mtx{\Lambda}$ is an $mxm$ diagonal matrix whose entries are the eigenvalues of $\mtx{A}^{*}\mtx{A}$.

  So then,
  \begin{align*}
    |\lambda_{\mathrm{min}}| &= \|\mtx{A}\vct{v}_{\mathrm{min}}\|_2&\quad\text{(let $\vct{v}_{\mathrm{min}}$ be a unit e.v. of $\mtx{A} \to \lambda_{\mathrm{min}}$)}\\
    |\lambda_{\mathrm{min}}|^2 &= \|\mtx{A}\vct{v}_{\mathrm{min}}\|_2^2&\quad\text{(squaring both sides)}\\
    &= \vct{v}_{\mathrm{min}}^{*}\mtx{A}^{*}\mtx{A}\vct{v}_{\mathrm{min}}&\quad\text{(def. of vector 2-norm)}\\
    &\geq \inf_{\|\vct{x}\| = 1}{\vct{x}^{*}\mtx{A}^{*}\mtx{A}\vct{x}}&\quad\text{(since $\vct{v}_{\mathrm{min}}$ is min. eigenvalue)}\\
    &= \inf_{\|\vct{x}\| = 1}{\vct{x}^{*}\mtx{U}\mtx{\Lambda}\mtx{U}^{*}\vct{x}}&\quad\text{($\mtx{\Lambda}_{i,i} = \lambda_i(\mtx{A}^{*}\mtx{A})$)}\\
    &= \inf_{\|\vct{y}\| = 1}{\vct{y}^{*}\mtx{\Lambda}\vct{y}}&\quad\text{(let $\vct{y} \equiv \mtx{U}^{*}\vct{x}$)}\\
    &= \lambda_{\mathrm{min}}(\mtx{A}^{*}\mtx{A})&\quad\text{(def. of $\lambda_{\mathrm{min}}$ - see (4) below)}\\
    &= \sigma_{\mathrm{min}}(\mtx{A})^2&\quad\text{(def. of singular value)}\\
  \end{align*}

  Thus, $|\lambda_{\mathrm{min}}(\mtx{A})|^2 \geq \sigma_{\mathrm{min}}(\mtx{A})^2 \implies |\lambda_{\mathrm{min}}(\mtx{A})| \geq \sigma_{\mathrm{min}}(\mtx{A})$.

  The same argument can be applied to show that $|\lambda_{\mathrm{max}}(\mtx{A})| \leq \sigma_{\mathrm{max}}(\mtx{A})$, which shows that
  \begin{equation*}
    m = n \Rightarrow \sigma_{\mathrm{min}}(\mtx{A}) \leq \left|\lambda_{i}\left(\mtx{A}\right)\right| \leq \sigma_{\mathrm{max}}\left(\mtx{A}\right), \forall i 
  \end{equation*}

\subsection*{(d) [5 pts]}
  \begin{equation*}
    \sigma_{\mathrm{max}}\left(\mtx{A}\right) = 1 / \sigma_{\mathrm{min}}\left(\mtx{A}^{-1}\right)\text{, if $\mtx{A}^{-1}$ exists}
  \end{equation*}

  We can decompose any matrix $\mtx{A} \in \R^{mxn}$ into components $\mtx{A} = \mtx{U} \mtx{\Sigma} \mtx{V}^T$, where $\mtx{U}$ and $\mtx{V}^T$ are orthogonal, and $\mtx{\Sigma} \in \R^{mxn}$ is a diagonal matrix whose entries $\mtx{\Sigma}_{i,i}$ contain all of the singular values of $\mtx{A}$.
  Since $\mtx{A}$ is invertible, all of its singular values will be nonzero, and so $\mtx{\Sigma}_{i,i} \neq 0, \forall i$.

  If we invert both sides of this equality, we get $\mtx{A}^{-1} = \mtx{V} \mtx{\Sigma}^{-1} \mtx{U}^T$, which holds since we are given that $\mtx{A}^{-1}$ exists.
  $\mtx{\Sigma}^{-1}$ also exists, since it is a diagonal matrix with no zeros on its diagonal.
  Its (also nonzero) diagonal entries are the singular values of $\mtx{A}^{-1}$, and its inverse can be found by inverting each of its diagonal elements (a property not proven here):

  \begin{align*}
    \mtx{\Sigma} &=
    \begin{bmatrix}
      \sigma_{1}(\mtx{A}) &  & \\ 
      & \ddots & \\ 
      &  & \sigma_{n}(\mtx{A})\\
      & \dots
    \end{bmatrix}\\
    \mtx{\Sigma}^{-1} &=
    \begin{bmatrix}
      \dfrac{1}{\sigma_{1}(\mtx{A})} &  & \\ 
      & \ddots & \\ 
      &  & \dfrac{1}{\sigma_{n}(\mtx{A})}\\
      & \dots
    \end{bmatrix}.
  \end{align*}

  Thus, for any $1 \leq i \leq n$, $\sigma_i(\mtx{A}) = \mtx{\Sigma}_{i,i} = \dfrac{1}{(\mtx{\Sigma}^{-1})_{i,i}} = \dfrac{1}{\sigma_i(\mtx{A}^{-1})}$.

  Finally, let $j = \arg\!\max\limits_{i}{\Sigma_{i,i}}$.
  Then $\sigma_j(\mtx{A}) = \sigma_{\mathrm{max}}(\mtx{A})$.
  Since each element in $\Sigma^{-1}$ is inverted, $j = \arg\!\min\limits_{i}{(\Sigma^{-1})_{i,i}}$, and so $\sigma_j(\mtx{A}^{-1}) = \sigma_{\mathrm{min}}(\mtx{A}^{-1})$, and so $\sigma_{\mathrm{max}}\left(\mtx{A}\right) = \dfrac{1}{\sigma_{\mathrm{min}}\left(\mtx{A}^{-1}\right)}$.

\subsection*{(e) [5 pts]}
  \begin{equation*}
    \text{Find the most general conditions on $m$, $n$, and $l$, under which } \sigma_{\mathrm{min}}\left(\mtx{A}\mtx{B}\right) \geq \sigma_{\mathrm{min}}\left(\mtx{A}\right) \sigma_{\mathrm{min}}\left(\mtx{B}\right).
  \end{equation*}

\section{Inverses [25 pts]}
We consider $\mtx{A} \in \R^{m \times m}$ and assume that $\mtx{A}^{-1}$ exists. 
You may use without proof that if $\mtx{A}$ is symmetric, its largest and smallest eigenvalues are characterized as 
\begin{equation*}
   \lambda_{\mathrm{max}}\left(\mtx(A)\right) = \sup \limits_{\vct{x} \in \R^m \setminus \{\vct{0}\}} \frac{\vct{x}^T \mtx{A}\vct{x}}{\|\vct{x}\|_2^2}
\end{equation*}
and 
\begin{equation*}
   \lambda_{\mathrm{min}}\left(\mtx(A)\right) = \inf \limits_{\vct{x} \in \R^m \setminus \{\vct{0}\}} \frac{\vct{x}^T \mtx{A}\vct{x}}{\|\vct{x}\|_2^2}
\end{equation*}


\subsection*{(a) [5 pts]}
Show that 
\begin{equation}
  \left\|\mtx{A}^{-1}\right\|_2^{-1} = \inf \limits_{\vct{x} \in \R^m \setminus \{\vct{0}\}} \frac{\|\mtx{A}\vct{x}\|_2}{\|\vct{x}\|_2}
\end{equation}

\begin{align*}
  \mtx{A} &= \mtx{U} \mtx{\Sigma} \mtx{V}^T&\quad\text{(SVD decomposition of $\mtx{A}$)}\\
  \mtx{A}^{-1} &= \mtx{V} \mtx{\Sigma}^{-1} \mtx{U}^T&\quad\text{(invert both sides)}\\
  \left\|\mtx{A}^{-1}\right\|_2 &= \left\|\mtx{V} \mtx{\Sigma}^{-1} \mtx{U}^T\right\|_2&\quad\text{(take the 2-norm of both sides)}\\
  &= \sigma_{\mathrm{max}}\left(\mtx{V} \mtx{\Sigma}^{-1} \mtx{U}^T\right)&\quad\text{(since $\left\|\mtx{B}\right\|_2 = \sigma_{\mathrm{max}}\left(\mtx{B}\right)$ for any matrix $\mtx{B}$)}\\
  &= \sigma_{\mathrm{max}}\left(\mtx{\Sigma}^{-1}\right)&\quad\text{(since $\mtx{V}$ and $\mtx{U}$ are orthogonal)}\\
  &= \frac{1}{\sigma_{\mathrm{min}}\left(\mtx{\Sigma}\right)}&\quad\text{((3d) above)}\\
  &= \frac{1}{\sigma_{\mathrm{min}}\left(\mtx{A}\right)}&\quad\text{(def. of $\mtx{\Sigma}$)}\\
  \left\|\mtx{A}^{-1}\right\|_2^{-1} &= \sigma_{\mathrm{min}}\left(\mtx{A}\right)&\quad\text{(invert both sides)}\\
  &=  \inf \limits_{\vct{x} \in \R^m \setminus \{\vct{0}\}} \frac{\|\mtx{A}\vct{x}\|_2}{\|\vct{x}\|_2}&\quad\text{(def. of $\sigma_\mathrm{min}$)}
\end{align*}

\subsection*{(b) [5 pts]}
Show that 
\begin{equation}
  \left\|\mtx{A}^{-1}\right\|_2^{-1} \geq \inf \limits_{\vct{x} \in \R^m \setminus \{\vct{0}\}} \frac{|\vct{x}^T \mtx{A}\vct{x}|}{\|\vct{x}\|_2^2}
\end{equation}

\begin{align*}
  \left\|\mtx{A}^{-1}\right\|_2^{-1} &= \inf \limits_{\vct{x} \in \R^m \setminus \{\vct{0}\}} \frac{\|\mtx{A}\vct{x}\|_2}{\|\vct{x}\|_2}&\quad\text{(from above)}\\
  &= \inf \limits_{\vct{x} \in \R^m \setminus \{\vct{0}\}} \frac{\|\mtx{A}\vct{x}\|_2\|\vct{x}\|_2}{\|\vct{x}\|_2^2}&\quad\text{(multiply num. and dem. by $\|\vct{x}\|_2$)}\\
  &= \inf \limits_{\vct{x} \in \R^m \setminus \{\vct{0}\}} \frac{\|\vct{x}^T \mtx{A}\vct{x}\|_2}{\|\vct{x}\|_2^2}&\quad\text{($\|\vct{x}^T \mtx{A}\vct{x}\|_2 = \|\mtx{A}\vct{x}\|_2 \|\vct{x}\|_2$)}\\
  &\geq \inf \limits_{\vct{x} \in \R^m \setminus \{\vct{0}\}} \frac{|\vct{x}^T \mtx{A}\vct{x}|}{\|\vct{x}\|_2^2}&\quad\text{($|\vct{x}^T \mtx{A}\vct{x}| \le \|\vct{x}^T \mtx{A}\vct{x}\|_2$)}
\end{align*}

\subsection*{(c) [5 pts]}

Show that for symmetric matrices $\mtx{M}, \mtx{N} \in \R^{m \times m}$, 
\begin{equation}
  \lambda_{\mathrm{min}}(\mtx{M} + \mtx{N}) \geq \lambda_{\mathrm{min}}\left(\mtx{M}\right) + \lambda_{\mathrm{min}}\left(\mtx{N}\right)
\end{equation}

To start, we will substitute the given expression for $\lambda_{\mathrm{min}}$, $\lambda_{\mathrm{min}}\left(\mtx{A}\right) = \inf \limits_{\vct{x} \in \R^m \setminus \{\vct{0}\}} \dfrac{\vct{x}^T \mtx{A}\vct{x}}{\|\vct{x}\|_2^2}$, for the left-side term, $\lambda_{\mathrm{min}}(\mtx{M} + \mtx{N})$:

\begin{align*}
  \lambda_{\mathrm{min}}(\mtx{M} + \mtx{N}) &= \inf \limits_{\vct{x} \in \R^m \setminus \{\vct{0}\}} \dfrac{\vct{x}^T \left(\mtx{M} + \mtx{N}\right)\vct{x}}{\|\vct{x}\|_2^2}\\
  &= \inf \limits_{\vct{x} \in \R^m \setminus \{\vct{0}\}} \dfrac{\vct{x}^T \mtx{M}\vct{x} + \vct{x}^T\mtx{N}\vct{x}}{\|\vct{x}\|_2^2}\\
  &= \inf \limits_{\vct{x} \in \R^m \setminus \{\vct{0}\}} \left(\dfrac{\vct{x}^T \mtx{M}\vct{x} }{\|\vct{x}\|_2^2} + \dfrac{\vct{x}^T\mtx{N}\vct{x}}{\|\vct{x}\|_2^2}\right)\\
\end{align*}

Next, restate the given left- and right-side terms, using an unknown inequality, "$?$", as a placeholder:

\begin{align*}
  \lambda_{\mathrm{min}}(\mtx{M} + \mtx{N}) &\q \lambda_{\mathrm{min}}\left(\mtx{M}\right) + \lambda_{\mathrm{min}}\left(\mtx{N}\right)\\
  \inf \limits_{\vct{x} \in \R^m \setminus \{\vct{0}\}} \left(\dfrac{\vct{x}^T \mtx{M}\vct{x} }{\|\vct{x}\|_2^2} + \dfrac{\vct{x}^T\mtx{N}\vct{x}}{\|\vct{x}\|_2^2}\right) &\q  \inf \limits_{\vct{y} \in \R^m \setminus \{\vct{0}\}} \left(\dfrac{\vct{y}^T \mtx{M}\vct{y}}{\|\vct{y}\|_2^2}\right) + \inf \limits_{\vct{z} \in \R^m \setminus \{\vct{0}\}} \left(\dfrac{\vct{z}^T \mtx{N}\vct{z}}{\|\vct{z}\|_2^2}\right)\\
\end{align*}

To determine what the unknown inequality, "$?$", must be, we observe that the left side is the \text{same expression} as the right side, except that it is optimizing to find a \textit{single} infimum value for both terms, while the right side is jointly optimizing for \textit{two separate} infimums - one for each term.
Another way of thinking about this is that the right side is \textit{less constrained} than the left in its "search" for a minimum (real) value - its "search space" is a superset of that of the left side, since it's allowed to test different vectors $\vct{y}$ and $\vct{z}$ on the left and right terms, respectively, rather than being constrained to test the same $\vct{y} = \vct{z}$ as the left side is.

Also, note that this unknown inequality "$?$", must allow for the case of equivalence, for example when $\mtx{M} = \mtx{N}$.

Although this argument is hand wavy, I hope that it is sufficient to convince the reader (grader), that the inequality "$?$" must be "$\geq$".

\subsection*{(d) [5 pts]}
In the same setting as (c), show that 
\begin{equation}
  \lambda_{\mathrm{max}}(\mtx{M} + \mtx{N}) \leq \lambda_{\mathrm{max}}\left(\mtx{M}\right) + \lambda_{\mathrm{max}}\left(\mtx{N}\right)
\end{equation}

Using the same approach as (c), we substitute the given expression for $\lambda_{\mathrm{max}}$,

\begin{equation*}
   \lambda_{\mathrm{max}}\left(\mtx{A}\right) = \sup \limits_{\vct{x} \in \R^m \setminus \{\vct{0}\}} \frac{\vct{x}^T \mtx{A}\vct{x}}{\|\vct{x}\|_2^2},
\end{equation*}

and determine the unknown inequality, "$?$":

\begin{align*}
  \lambda_{\mathrm{max}}(\mtx{M} + \mtx{N}) &\q \lambda_{\mathrm{max}}\left(\mtx{M}\right) + \lambda_{\mathrm{max}}\left(\mtx{N}\right)\\
  \sup \limits_{\vct{x} \in \R^m \setminus \{\vct{0}\}} \left(\dfrac{\vct{x}^T \mtx{M}\vct{x} }{\|\vct{x}\|_2^2} + \dfrac{\vct{x}^T\mtx{N}\vct{x}}{\|\vct{x}\|_2^2}\right) &\q  \sup \limits_{\vct{y} \in \R^m \setminus \{\vct{0}\}} \left(\dfrac{\vct{y}^T \mtx{M}\vct{y}}{\|\vct{y}\|_2^2}\right) + \sup \limits_{\vct{z} \in \R^m \setminus \{\vct{0}\}} \left(\dfrac{\vct{z}^T \mtx{N}\vct{z}}{\|\vct{z}\|_2^2}\right)\\
\end{align*}

Using the exact same argument as above, but noting that the functions being optimized here are supremums instead of infimums, this inequality "$?$" must be "$\leq$".

\subsection*{(e) [5 pts]}
For $\mtx{A}$ symmetric positive definite, show that 
\begin{equation*}
  \|\mtx{A}^{-1}\|_{2}^{-1} = \inf \limits_{\vct{x} \in \R^m \setminus \{\vct{0}\}} \frac{\vct{x}^T \mtx{A} \vct{x}}{\|\vct{x}\|^2_2}
\end{equation*}

In (4a) above, we showed that 

\begin{equation*}
  \left\|\mtx{A}^{-1}\right\|_2^{-1} = \sigma_{\mathrm{min}}\left(\mtx{A}\right).
\end{equation*}

For symmetric matrices, the singular values are the absolute values of the eigenvalues.

\section{Oblivion [25 pts]}
\subsection*{(a) [5 pts]} 
Have a look at the function \texttt{add\_to\_A\_B\_times\_C!} in \texttt{HW2\_your\_code.jl}. 
This function adds the product of the input variables \texttt{B} and \texttt{C} to the input variable \texttt{A}. 
It is written such as to use the optimal loop order, and employs multiple optimizations by means of the \texttt{@turbo} macro.

Go to section (a) of the file \texttt{HW2\_driver.jl} to see how this function performs when compared to the built-in \texttt{mul!} function. 
If necessary, adapt the size of the matrix to your system. 
Hand in the file \texttt{performance\_per\_size.pdf} produced by the code with your homework. 

\subsection*{(b)locked [5 pts]} 
Now implement a blocked/tiled variant of \texttt{add\_to\_A\_B\_times\_C!} that takes an integer \texttt{bks} as a fourth input.
This method should perform the matrix multiplication by calling the original \texttt{add\_to\_A\_B\_times\_C!} on blocks of size (roughly) \texttt{bks} times \texttt{bks}.
Again, your code should be correct and avoid memory allocations as checked by section (b) of \texttt{HW2\_driver.jl}.

\subsection*{(c) [5 pts]}
Use section \texttt{HW2\_driver.jl} to try the performance of the blocked algorithm for different matrix sizes and block sizes. 
Describe your results, and for the most interesting set of parameters, hand in the resulting plot with your homework.
Describe and explain the results.

\subsection*{(d) oblivious [5 pts]}
We can sometimes obtain better performance by using a hierarchical algorithm. 
Complete the skeleton for \texttt{oblivious\_add\_to\_A\_B\_times\_C!} to obtain an algorithm that equally divides each of its input matrices into four parts and then recurses on the resulting eight subproblems, until one of the problems reaches a size below \texttt{bks}.
Again, make sure that your algorithm does not allocate memory and is correct using part (d) \texttt{HW2\_driver.jl}.

\subsection*{(e) [5 pts]} Use part (e) of \texttt{HW2\_driver.jl} to benchmark your new algorithm for different values of \texttt{bks}. 
Are you able to obtain an improvement over the blocked algorithm? 
Hand in the figure produced by part (e) of \texttt{HW2\_driver.jl} for what you consider the most interesting set of parameters.
Algorithms of this type are called ``cache-oblivious.'' 
Explain why this is the case and what could be the theoretical advantages of this particular cache-oblivious algorithm.

% 
% \subsection*{(b) Matrix-vector-multiplication [5 pts]} 
% Complete the function \texttt{u\_is\_A\_times\_v!(u, A, v)} in \texttt{HW1\_your\_code.jl} that overwrites the input vector \texttt{u} with the product of the input matrix \texttt{A} and the input vector \texttt{v}.
% 
% \subsection*{(c) Matrix-matrix-multiplication [5 pts]} 
% Complete the function \texttt{A\_is\_B\_times\_C!(A, B, C)} in \texttt{HW1\_your\_code.jl} that overwrites the input matrix \texttt{A} with the product of the input matrices \texttt{B} and \texttt{C}.
% 
% \subsection*{(d) Testing [5 pts]}
% From the directory \texttt{HW1\_CODE}, run the command 
% \begin{verbatim}
%   julia --project=. HW1_driver.jl
% \end{verbatim}
% to test your code. Here, the \texttt{-\,-\,project=.} tells Julia to use \texttt{Manifest.toml} and \texttt{Project.toml} determine which version (if any) of packages to use.
% Make sure that your code passes all \texttt{@assert} statements, which test your functions against Julia's built-in functions.
% 
% \subsection*{(e) Optimization [5 pts]} 
% Make sure that your code does not allocate any memory, as evidenced by the \texttt{@btime} calls in \texttt{HW1\_driver.jl} returning \texttt{(0\,allocations:\,0\,bytes)}.
% This is important for performance reasons since allocating memory may be orders of magnitudes slower than floating-point arithmetic. 
% Now try reordering the for-loops in your implementations from parts (a) and (b) and observe the resulting timings provided by \texttt{@btime}. 
% Which order leads to the best and worst performance? 
% What are the corresponding wall-clock times as measured by \texttt{@btime}? 

















%\bibliographystyle{plain}
%\bibliography{temp,externalPapers,groupPapers}

\end{document}
