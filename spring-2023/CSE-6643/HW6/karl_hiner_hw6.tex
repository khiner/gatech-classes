\documentclass[twoside,10pt]{article}
\input{macro.tex}

\begin{document}

\title{CSE 6643 Homework 6}
\author{Karl Hiner, Spring 2023}
\date{}
\maketitle

\section{Convergence of QR iteration [50 pts]}
  In this problem, we consider the convergence rate of the QR algorithm with a single-shift strategy. We consider a real matrix $\mtx{A} \in \R^{m \times m}$. 
  The QR iteration can be written as follows:
  \begin{align}
    \mtx{A}^{(0)} &= \mtx{A} \\
    \mtx{A}^{(k)} &= \mu_k \Id + \mtx{Q}_k \mtx{R}_k,\\
    \mtx{A}^{(k+1)} &= \mtx{R}_k \mtx{Q}_k + \mu_k \Id.
  \end{align}
  If we choose $\mu_k = \mtx{A}_{m, m}^{(k)}$ to be the bottom-right entry of the matrix $\mtx{A}^{(k)}$, then this is called the \emph{single-shift QR iteration.}

  Prove the following results.
  You may use figures to illustrate your explanations. 
  \subsection*{(a) [10 pts]}
  Show that if $\mtx{A}^{(0)} = \mtx{A}$ is an upper Hessenberg matrix, then $\mtx{A}^{(k)}$ is upper Hessenberg for all $k \geq 0$.
  Thus, from now on, we always assume that the matrix $\mtx{A}$ is an upper Hessenberg matrix.

    \quad We use induction. For $k = 0$, $\mtx{A}^{(0)} = \mtx{A}$ is upper Hessenberg by assumption.
    
    Now, assume that $\mtx{A}^{(k)}$ is upper Hessenberg.
    To find $\mtx{A}^{(k+1)}$, we perform a QR factorization of $\mtx{A}^{(k)} - \mu_k \Id $.
    
    Since $\mtx{A}^{(k)} - \mu_k \Id$ is an upper Hessenberg matrix, the resulting $\mtx{Q}_k$ factor will also be upper Hessenberg.

    To show this, multiply both sides of $\mtx{Q}_k \mtx{R}_k = \mtx{A}^{(k)} - \mu_k \Id$ by $\mtx{R}_k^{-1}$ to get
    $$\mtx{Q}_k = (\mtx{A}^{(k)} - \mu_k \Id) \mtx{R}_k^{-1}.$$
    
    $\mtx{A}^{(k)}$ is upper Hessenberg by our inductive assumption.
    $\mu_k \Id$ is a diagonal matrix, so $\mtx{A}^{(k)} - \mu_k \Id$ is also upper Hessenberg.
    $\mtx{R}_k$ is an upper triangular matrix, so $\mtx{R}_k^{-1}$ is also upper triangular.
  
    Thus, $\mtx{Q}_k$ is the product of an upper Hessenberg matrix and an upper triangular matrix, and so it is also upper Hessenberg.
 
    Now, to show that $\mtx{A}^{(k+1)} = \mtx{R}_k \mtx{Q}_k + \mu_k \Id$ is upper Hessenberg, we need to prove that the entries below the first subdiagonal are zero.
    
    Consider entry $a_{i,j}^{(k+1)}$ of $\mtx{A}^{(k+1)}$, where $i > j + 1$:
    \begin{align*}
      \mtx{A}^{(k+1)} &= \mtx{R}_k \mtx{Q}_k + \mu_k \Id \\
      a_{i,j}^{(k+1)} &= (\mtx{R}_k \mtx{Q}_k)_{i,j} + (\mu_k \Id)_{i,j}\\
      &= (\mtx{R}_k \mtx{Q}_k)_{i,j}&\text{($\mu_k \Id$ diag. $\rightarrow (\mu_k \Id)_{i,j} = 0, \forall i \neq j$)}\\
      &= \sum_{p=1}^m r_{i,p}^{(k)} q_{p,j}^{(k)}
    \end{align*}
    Since $\mtx{R}_k$ is upper triangular, $r_{i,p}^{(k)} = 0$ for all $i > p$.
    Also, since $\mtx{Q}_k$ is upper Hessenberg, $q_{p,j}^{(k)} = 0$ for all $p > j + 1$.
    Combining these observations, we can conclude that $(\mtx{R}_k \mtx{Q}_k)_{i,j} = 0$ for all $i > j + 1$, and so $\mtx{A}^{(k+1)}$ is upper Hessenberg.

    Thus, we have shown that if $\mtx{A}^{(k)}$ is upper Hessenberg, then $\mtx{A}^{(k+1)}$ is also upper Hessenberg, which completes the induction.
 
  \subsection*{(b) [10 pts]}
  Prove that the total operation cost for each QR iteration is $O(m^2)$.

  \quad A single step of QR iteration involves a QR factorization and computing $\mtx{A}^{(k+1)}$ from the QR factors.

  \begin{enumerate}
    \item QR factorization:
    
    $\mtx{A}^{(k)} - \mu_k \Id$ is upper Hessenberg.
    We can compute its QR factorization using Givens rotations.
    Each Givens rotation zeros out one subdiagonal element in each column.
    Since there are $m-1$ nonzero subdiagonal elements, there will be $O(m-1)$ Givens rotations.
    Each Givens rotation requires $4m$ operations (2 multiplications and 2 additions per entry across two rows),
    giving a total cost of $4m(m-1) = O(m^2)$.
    \item Compute $\mtx{A}^{(k+1)} = \mtx{R}_k \mtx{Q}_k + \mu_k \Id$:
    
    $\mtx{R}_k \mtx{Q}_k$ is the product of an upper triangular matrix and an upper Hessenberg matrix, so we can compute the product using $O(m^2)$ operations.
    Adding the shift $\mu_k \Id$ takes $O(m)$ operations.
  \end{enumerate}

  Thus, the total operation cost for each QR iteration is $O(m^2) + O(m^2) + O(m) = O(m^2)$.

  \subsection*{(c) [10 pts]}
  In the QR step, we perform $m - 1$ Givens rotations on the matrix $\mtx{A}^{(k)} - \mu_k \Id$. 
  Suppose that after $m - 2$ Givens rotations, the bottom-right $2 \times 2$ sub-matrix of $\mtx{A}^{(k)} - \mu_k \Id$ is given by
  \begin{equation}
    \begin{pmatrix}
      a & b \\
      \varepsilon & 0
    \end{pmatrix}.
  \end{equation}
  Explain why the $(m, m)$ entry is $0$ at that stage, and prove that 
  \begin{equation}
    \mtx{A}_{m, m-1}^{(k + 1)} = - \frac{\varepsilon^2 b}{\varepsilon^2 + a^2}.
  \end{equation}

\quad Since we have applied $m-2$ Givens rotations, the $(m, m)$ entry is $0$ because each Givens rotation introduces a $0$ entry below the diagonal, and we have only one non-zero entry left in the last subdiagonal.

We are looking for the lower-left entry of the bottom-right $2\times2$ submatrix of the product $G^T(\mtx{A}^{(k)} - \mu_k \Id)G$, where $G$ is the Givens rotation matrix:
$$G \coloneqq \begin{pmatrix}
      c & -s \\
      s & c
\end{pmatrix},
$$
with $c \coloneqq \dfrac{a}{\sqrt{a^2 + \varepsilon^2}}$ and $s \coloneqq \dfrac{\varepsilon}{\sqrt{a^2 + \varepsilon^2}}$.

For brevity, let's denote the subscript $2\times2$ as the lower-right $2\times2$ submatrix of a matrix.

We have:
\begin{align*}
  \left(G^T(\mtx{A}^{(k)} - \mu_k \Id)G\right)_{2\times2} &= \left(G^T\mtx{A}^{(k)}G\right)_{2\times2} -\left(G^T \mu_k \Id G\right)_{2\times2}\\
  &=
  \begin{pmatrix}
      c & s \\
      -s & c
  \end{pmatrix}
  \begin{pmatrix}
      a & b \\
      \varepsilon & 0
  \end{pmatrix}
  \begin{pmatrix}
      c & -s \\
      s & c
  \end{pmatrix} +
  \begin{pmatrix}
    c & s \\
    -s & c
\end{pmatrix}
\begin{pmatrix}
    \mu_k & 0 \\
    0 & \mu_k
\end{pmatrix}
\begin{pmatrix}
    c & -s \\
    s & c
\end{pmatrix} 
\end{align*}

Note that we ultimately only care about the lower-left entry of the bottom-right $2\times2$ submatrix.

We can see the right term will contribute nothing to this entry:
$$
\begin{pmatrix}
  c & s \\
  -s & c
\end{pmatrix}
\begin{pmatrix}
  \mu_k & 0 \\
  0 & \mu_k
\end{pmatrix}
\begin{pmatrix}
  c & -s \\
  s & c
\end{pmatrix} = 
\begin{pmatrix}
  \cdots & \cdots \\
  \mu_k(-sc + cs) & \cdots 
\end{pmatrix} = 
\begin{pmatrix}
  \cdots & \cdots \\
  0 & \cdots 
\end{pmatrix}
$$

This leaves us with the left term to compute:
\begin{align*}
\begin{pmatrix}
  c & s \\
  -s & c
\end{pmatrix}
\begin{pmatrix}
  a & b \\
  \varepsilon & 0
\end{pmatrix}
\begin{pmatrix}
  c & -s \\
  s & c
\end{pmatrix}\\
\begin{pmatrix}
  ac + s\varepsilon & cb \\
  -sa + c\varepsilon & -sb
\end{pmatrix}
\begin{pmatrix}
  c & -s \\
  s & c
\end{pmatrix}\\
\begin{pmatrix}
  \cdots & \cdots \\
  (-sa + c\varepsilon)c + (-sb)s & \cdots
\end{pmatrix}
\end{align*}

Simplifying the lower-left entry and substituting for $c$ and $s$,
\begin{align*}
(-sa + c\varepsilon)c + (-sb)s &= -csa + c^2\varepsilon - s^2b\\
&= -\dfrac{a^2\varepsilon}{a^2 + \varepsilon^2} + \dfrac{a^2\varepsilon}{a^2 + \varepsilon^2} - \frac{\varepsilon^2 b}{\varepsilon^2 + a^2}\\
&= - \frac{\varepsilon^2 b}{\varepsilon^2 + a^2},
\end{align*}
which is our desired result.

  \subsection*{(d) [10 pts]} 
  Based on the previous result, explain why we can expect the single-shift QR algorithm to converge quadratically (provided that it is converging). 

  \quad In part (c), we derived that the off-diagonal entry of the bottom-right $2 \times 2$ sub-matrix of $\mtx{A}^{(k+1)}$ is given by
  $$-\frac{\varepsilon^2 b}{\varepsilon^2 + a^2}.$$
  The $\varepsilon^2$ is in the numerator means that as $\varepsilon \to 0$, the off-diagonal entry $\mtx{A}_{m, m-1}^{(k + 1)}$ will also approach $0$.
  That is, the off-diagonal entry decreases quadratically with respect to $\varepsilon$.
  Since the QR algorithm aims to drive the off-diagonal entries to $0$ to reveal the eigenvalues along the diagonal, this suggests the single-shift QR algorithm will converge quadratically if it converges at all.

  \subsection*{(e) [10 pts]}
  We showed that the single-shift QR algorithm converges quite fast if the guess is sufficiently accurate. 
  However, its convergence is not guaranteed.
  Give an example in which the single-shift QR algorithm fails to converge, and explain why.

\quad Consider the following matrix:

$$\mtx{A} = \begin{pmatrix}
  0 & 1 \\
  1 & 0
\end{pmatrix}$$

The true eigenvalues of $\mtx{A}$ are $\lambda_1 = 1$ and $\lambda_2 = -1$, and so $\left|\lambda_1\right| = \left|\lambda_2\right| = 1$.

Thus, the single-shift QR algorithm cannot decide which direction to move its estimate for the eigenvalues, and so it cannot proceed forward.

To see this, consider the first iteration of the single-shift QR algorithm applied to $\mtx{A}$.

\begin{itemize}
\item Initialize $\mtx{T}_0 = \mtx{A}$.
\item Since the diagonal entries are both zero, we choose $\mu_1 = 0$ as the shift (our "eigenvalue estimate").
\item Then, we take the QR factorization of $\mtx{T}_0 - \mu_1 \Id = \mtx{T}_0$.
\end{itemize}

We can already see that we can make no progress.
Furthermore, note that the off-diagonals are not zero (or near-zero), and never will be, and so we cannot use deflation to split up the problem and make progress that way either.

As explained in class, we need a way to break the symmetry of the problem, and this is why we introduce the Wilkinson Shift.

\section{Deflation upon Convergence [20 pts]}
Consider an upper Hessenberg matrix $\mtx{H} \in \R^{m \times m}$ with eigenvalue $\lambda$. 
We define 
\begin{align}
  \mtx{H} - \lambda \Id &= \mtx{U}_1 \mtx{R}_1 \quad \text{(QR factorization)} \\ 
  \mtx{H}_1 &= \mtx{R}_1 \mtx{U}_1 + \lambda \Id.
\end{align}

\subsection*{(a) [10 pts]}
Prove that if $\mtx{H}_{i + 1, i} \neq  0, \forall 1 \leq i < m$ ($\mtx{H}$ is an unreduced Hessenberg matrix), then 
\begin{equation}
  \mtx{H}_1\left(m, \colon\right) = \lambda e_m^{T}. 
\end{equation}

  Considering the last row of both sides of the given equation (6):
  \begin{align}
    \mtx{U}_1(m, \colon) \mtx{R}_1 &= (\mtx{H} - \lambda \Id)(m, \colon) \\
    &= \begin{bmatrix} 0 & \cdots & 0 & h_{m,m-1} & h_{m,m} - \lambda \end{bmatrix}
  \end{align}
  
  The last row of $\mtx{H}_1(m, \colon)$ will be
  \begin{align*}
    \mtx{H}_1(m, \colon) &= \mtx{R}_1 \mtx{U}_1(m, \colon) + \lambda \vct{e}_m^T
  \end{align*}
  
  Since $\mtx{R}_1$ is upper triangular and $\mtx{U}_1(m, i) = 0$ for $1 \leq i < m$, the only non-zero term in the product $\mtx{R}_1 \mtx{U}_1(m, \colon)$ comes from $i = m$:
  \begin{align*}
    \mtx{R}_1 \mtx{U}_1(m, \colon) &= \mtx{R}_1(m, m) \mtx{U}_1(m, m)
  \end{align*}
  
  From equations (10), we know that
  \begin{equation}
    \mtx{R}_1(m, m) \mtx{U}_1(m, m) = h_{m, m} - \lambda.
  \end{equation}
  
  Combining these results, we have
  $$\mtx{H}_1(m, \colon) = (h_{m, m} - \lambda) \vct{e}_m^T + \lambda \vct{e}_m^T = \lambda \vct{e}_m^T.$$
  
\subsection*{(b) [10 pts]}
Explain the connection between this result and the process of deflation in the QR iteration algorithm. 

\quad When an off-diagonal entry of the matrix in the QR iteration converges to near-zero, we can "deflate" the matrix by dividing it into two smaller matrices, and applying the QR iteration to each smaller matrix separately.

In problem (a), we found that if $\mtx{H}_{m, m-1}$ converges to zero, the last row of the matrix $\mtx{H}_1$ becomes $\lambda e_m^T$. This means that the matrix $\mtx{H}_1$ can be partitioned into two smaller matrices, one of size $(m-1) \times (m-1)$ and the other of size $1 \times 1$. The $1 \times 1$ matrix contains the converged eigenvalue $\lambda$ and is deflated from the rest of the matrix. The QR iteration can then be applied to the smaller $(m-1) \times (m-1)$ matrix.

\section{An implicit QR Factorization [15 bonus pts]}
Denote $\mtx{H} = \mtx{H}_1$, and assume we generate a sequence of matrices $\mtx{H}_k$ via 
\begin{equation}
  \mtx{H}_k - \mu_k \Id = \mtx{U}_k \mtx{R}_k, \quad \mtx{H}_{k + 1} = \mtx{R}_k \mtx{U}_k + \mu_k \Id. 
\end{equation}
Prove that 
\begin{equation}
  \left(\mtx{U}_1 \cdots \mtx{U}_j\right)\left(\mtx{R}_j \cdots \mtx{R}_1\right) = \left(\mtx{H} - \mu_{j} \Id\right) \cdots \left(\mtx{H} - \mu_1 \Id\right). 
\end{equation}
This result shows that we are implicitly computing a QR factorization of 
\begin{equation}
  \left(\mtx{H} - \mu_j \Id\right) \cdots \left(\mtx{H} - \mu_1 \Id\right).   
\end{equation}

\quad We will proove this result by induction on $k$.

\textbf{Base case ($k = 1$):}
\begin{align*}
  \mtx{U}_k \mtx{R}_k &= \mtx{H}_k - \mu_k \Id&\text{(given)}\\
  \mtx{U}_1 \mtx{R}_1 &= \mtx{H}_1 - \mu_1 \Id&\text{(set $k = 1$)}\\
  \left(\mtx{U}_1\right)\left(\mtx{R}_1\right) &= \left(\mtx{H} - \mu_1 \Id\right)&\text{(since $\mtx{H} \coloneqq \mtx{H}_1$. QED for base case)}
\end{align*}

\textbf{Inductive step:}
Assume that the result holds for $k > 0$.

Define $\mtx{Q}_k \coloneqq \left(\mtx{U}_1 \cdots \mtx{U}_k\right)$, $\mtx{P}_k \coloneqq \left(\mtx{R}_k \cdots \mtx{R}_1\right)$, and $\mtx{G}_k \coloneqq \left(\mtx{H} - \mu_k \Id\right) \cdots \left(\mtx{H} - \mu_1 \Id\right)$.

Then, our inductive assumption is
\begin{align*}
  \left(\mtx{U}_1 \cdots \mtx{U}_k\right)\left(\mtx{R}_k \cdots \mtx{R}_1\right) &= \left(\mtx{H} - \mu_k \Id\right) \cdots \left(\mtx{H} - \mu_1 \Id\right)\\
  \mtx{Q}_k\mtx{P}_k &= \mtx{G}_k.
\end{align*}
We want to show that the result also holds for $k+1$.
That is, we want to show that
\begin{align*}
  \left(\mtx{U}_1 \cdots \mtx{U}_{k+1}\right)\left(\mtx{R}_{k+1} \cdots \mtx{R}_1\right) &= \left(\mtx{H} - \mu_{k+1} \Id\right) \cdots \left(\mtx{H} - \mu_1 \Id\right)\\
  \left(\mtx{U}_1 \cdots \mtx{U}_{k} \mtx{U}_{k+1}\right)\left(\mtx{R}_{k+1} \mtx{R}_{k} \cdots \mtx{R}_1\right) &=  \left(\mtx{H} - \mu_{k+1} \Id\right) \left(\mtx{H} - \mu_k \Id\right) \cdots \left(\mtx{H} - \mu_1 \Id\right)\\
  \mtx{Q}_k \mtx{U}_{k+1}\mtx{R}_{k+1} \mtx{P}_k &= \left(\mtx{H} - \mu_{k+1} \Id\right) \mtx{G}_k.
\end{align*}

First, observe the following relation:
\begin{align*}
  \mtx{H}_k - \mu_k \Id &= \mtx{U}_k \mtx{R}_k&\text{(given)}\\
  \left(\mtx{U}_k\right)^T \mtx{H}_k \mtx{U}_k &=\left(\mtx{U}_k\right)^T \left(\mtx{U}_k \mtx{R}_k + \mu_k \Id\right) \mtx{U}_k&\text{(mult. left and right by $\left(\mtx{U}_k\right)^T$ and $\mtx{U}_k$)}\\
  &= \mtx{R}_k \mtx{U}_k + \mu_k \Id&\text{(since $\mtx{U}_k$ is orthonormal)}\\
  \left(\mtx{U}_k\right)^T \mtx{H}_k \mtx{U}_k &= \mtx{H}_{k+1}&\text{(by definition of $\mtx{H}_{k+1}$)}
\end{align*}


\section{QR with Shifts [30 pts]}
\subsection*{(a) Almost upper triangular [7.5 pts]} 
Go to section (a) of the file \texttt{HW6\_your\_code.jl} and implement a function that reduces a symmetric matrix $\mtx{A} \in \mathbb{R}^{m\times m}$ to Hessenberg form using Householder reflections. You should end up with a matrix $\mtx{T}$ in Hessenberg form. Your algorithm should operate in place, overwriting the input matrix and not allocating additional memory. 

\subsection*{(b) Givens [7.5 pts]} 
Go to section (b) of the file \texttt{HW6\_your\_code.jl} and implement a function that runs a single iteration of the unshifted QR algorithm. Your function should take $\mtx{T}_{k}$ in Hessenberg form as an input and compute $\mtx{T}_{k+1}$ also in Hessenberg form. You should use Givens rotations to implement QR-factorization on $\mtx{T}_{k}$.

\subsection*{(c) Single-Shift vs. Wilkson Shifts [7.5 pts]}
Go to section (c) of the file \texttt{HW6\_your\_code.jl} and implement a function that runs the practical QR iteration with both the Single-Shift and Wilkinson Shift. Your function should have an input that allows you to select which type of shift you want to use.  Your implementation should include deflation and a reasonable criteria for when to implement deflation and terminate your QR iterations. You can use your function from part (b) to do the QR iteration at each step.

\subsection*{(d) Breaking symmetry [7.5 pts]}
Go to section (d) of the file \texttt{HW6\_your\_driver.jl} and design an experiment that evaluates your practical QR algorithm with shifts. You should include a semi-log plot showing the rate of convergence of your algorithm using Single-Shift and Wilkinson Shift. Compare the results with the rate of convergence you expected to see for both cases. Do you have a preference between the Wilkinson shift or the Rayleigh shift?  If so which one do you prefer and why?

\end{document}